{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Mini deep-learning framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to copy paste everything here to test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\" File to solve the second miniproject which is the design of a framework \"\"\"\n",
    "\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import empty\n",
    "\n",
    "__author__ = 'Eugène Lemaitre, Natalie Bolón Brun, Louis Munier'\n",
    "__version__ = '0.1'\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Linear import Linear\n",
    "from Activation import Tanh, Relu, Sigmoid\n",
    "from Loss import LossMSE, CrossEntropy\n",
    "from Optimizers import Optimizers, Sgd, DecreaseSGD\n",
    "from Sequential import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_number_error(output_one_hot, target_one_hot):\n",
    "    output = output_one_hot.argmax(dim=1)\n",
    "    target = target_one_hot.argmax(dim=1)\n",
    "    nb_of_error = (output != target).sum()\n",
    "    return nb_of_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_disc_set(nb):\n",
    "    X = empty(nb,2).uniform_(0,1)\n",
    "    Y = empty(X.size())\n",
    "    \n",
    "    Y[:,0] = ((X - 0.5).norm(dim=1)  >  math.sqrt(1/(2*math.pi))).type(torch.LongTensor)\n",
    "    Y[:,1] = ((X - 0.5).norm(dim=1)  <=  math.sqrt(1/(2*math.pi))).type(torch.LongTensor)\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "def plot_disc(data_in, data_target, title):\n",
    "    plt.scatter(data_in[(data_target[:,1]==1),0] ,data_in[(data_target[:,1]==1),1], color = \"c\", s=20)\n",
    "    plt.scatter(data_in[(data_target[:,1]==0),0] ,data_in[(data_target[:,1]==0),1], color = \"g\", s=20)\n",
    "    plt.title(title)\n",
    "    plt.legend([\"1\", \"0\"] )\n",
    "    plt.show()\n",
    "    \n",
    "def plot_result(data_in, data_target, data_class):\n",
    "    one_id_as_one = torch.mul( (data_target[:,1]==1), (data_class ==1 ))\n",
    "    one_id_as_zero = torch.mul( (data_target[:,1]==1), (data_class ==0 ))\n",
    "    zero_id_as_one = torch.mul( (data_target[:,1]==0), (data_class ==1 ))\n",
    "    zero_id_as_zero = torch.mul( (data_target[:,1]==0), (data_class ==0 ))\n",
    "    \n",
    "    plt.scatter(data_in[one_id_as_one,0] ,data_in[one_id_as_one,1], color = \"c\", s=20)\n",
    "    plt.scatter(data_in[zero_id_as_zero,0] ,data_in[zero_id_as_zero,1], color = \"g\", s=20)\n",
    "    plt.scatter(data_in[one_id_as_zero,0] ,data_in[one_id_as_zero,1], color = \"r\", s=20)\n",
    "    plt.scatter(data_in[zero_id_as_one,0] ,data_in[zero_id_as_one,1], color = \"y\", s=20)\n",
    "    \n",
    "    plt.title(\"Result on train data\")\n",
    "    plt.legend([\"1 id as 1\", \"0 id as 0\", \"1 id as 0\", \"0 id as 1\"] )\n",
    "    plt.show()\n",
    "    \n",
    "def plot_loss(epochs, loss):\n",
    "    plt.plot(epochs, loss)\n",
    "    plt.title(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sample_number = 1000\n",
    "train_input, train_target = generate_disc_set(Sample_number)\n",
    "test_input, test_target = generate_disc_set(Sample_number)\n",
    "\n",
    "\n",
    "plot_disc(train_input, train_target, \"Train data before normalization\")\n",
    "\n",
    "mu, std = train_input.mean(0), train_input.std(0)\n",
    "train_input.sub_(mu).div_(std)\n",
    "test_input.sub_(mu).div_(std)\n",
    "\n",
    "plot_disc(train_input, train_target, \"Train data after normalization\")\n",
    "plot_disc(test_input, test_target, \"Test data after normalization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: No dropout, constant learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_nb = 25\n",
    "std = 0.1\n",
    "eta = 3e-1\n",
    "batch_size = 200\n",
    "epochs_number = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "linear_1 = Linear(2, hidden_nb)\n",
    "relu_1 = Relu()\n",
    "linear_2 = Linear(hidden_nb, hidden_nb)\n",
    "relu_2 = Relu()\n",
    "linear_3 = Linear(hidden_nb, hidden_nb)\n",
    "relu_3 = Relu()\n",
    "linear_4 = Linear(hidden_nb, 2)\n",
    "loss = CrossEntropy()\n",
    "\n",
    "model_1 = Sequential(linear_1, relu_1, linear_2, relu_2, linear_3, relu_3, linear_4, loss=CrossEntropy()) \n",
    "       \n",
    "\n",
    "\n",
    "my_loss_1 = []\n",
    "\n",
    "model_1.normalize_parameters(mean=0, std=std)\n",
    "optimizer = Sgd()\n",
    "\n",
    "for epochs in range(0, epochs_number):\n",
    "    for b in range(0, Sample_number, batch_size):\n",
    "        output = model_1.forward(train_input.narrow(0, b, batch_size))\n",
    "        loss_value =  model_1.compute_loss(output, train_target.narrow(0, b, batch_size))\n",
    "        model_1.backward()\n",
    "        \n",
    "        optimizer(model_1.sequence, eta=eta)\n",
    "    \n",
    "    if epochs%50 == 0:\n",
    "        print(epochs, \": \", loss_value.item())\n",
    "    my_loss_1.append(loss_value.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_1.forward(train_input)\n",
    "l = model_1.compute_loss(output, train_target)\n",
    "\n",
    "print(\"\\n \")\n",
    "print(\"Loss: \", l.item())\n",
    "print(\"Number of errors: \", compute_number_error(output, train_target).item())\n",
    "print(\"\\n \")\n",
    "\n",
    "\n",
    "id_class_train = output.argmax(dim=1)\n",
    "plot_result(train_input, train_target, id_class_train)\n",
    "plot_loss(range(0, epochs_number), my_loss_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_1.forward(test_input)\n",
    "l = model_1.compute_loss(output, test_target)\n",
    "\n",
    "print(\"\\n \")\n",
    "print(\"Loss: \", l.item())\n",
    "print(\"Number of errors: \", compute_number_error(output, test_target).item())\n",
    "print(\"\\n \")\n",
    "\n",
    "\n",
    "id_class_test = output.argmax(dim=1)\n",
    "plot_result(test_input, test_target, id_class_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: No dropout, deacreasing learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "linear_1 = Linear(2, hidden_nb)\n",
    "relu_1 = Relu()\n",
    "linear_2 = Linear(hidden_nb, hidden_nb)\n",
    "relu_2 = Relu()\n",
    "linear_3 = Linear(hidden_nb, hidden_nb)\n",
    "relu_3 = Relu()\n",
    "linear_4 = Linear(hidden_nb, 2)\n",
    "loss = CrossEntropy()\n",
    "\n",
    "model_2 = Sequential(linear_1, relu_1, linear_2, relu_2, linear_3, relu_3, linear_4, loss=CrossEntropy()) \n",
    "       \n",
    "my_loss_2 = []\n",
    "\n",
    "model_2.normalize_parameters(mean=0, std=std)\n",
    "optimizer = DecreaseSGD()\n",
    "\n",
    "for epochs in range(0, epochs_number):\n",
    "    for b in range(0, Sample_number, batch_size):\n",
    "        output = model_2.forward(train_input.narrow(0, b, batch_size))\n",
    "        loss_value =  model_2.compute_loss(output, train_target.narrow(0, b, batch_size))\n",
    "        model_2.backward()\n",
    "        \n",
    "        optimizer(epochs, model_2.sequence, eta=eta)\n",
    "    \n",
    "    if epochs%50 == 0:\n",
    "        print(epochs, \": \", loss_value.item())\n",
    "    my_loss_2.append(loss_value.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_2.forward(train_input)\n",
    "l = model_2.compute_loss(output, train_target)\n",
    "\n",
    "print(\"\\n \")\n",
    "print(\"Loss: \", l.item())\n",
    "print(\"Number of errors: \", compute_number_error(output, train_target).item())\n",
    "print(\"\\n \")\n",
    "\n",
    "\n",
    "id_class_train = output.argmax(dim=1)\n",
    "plot_result(train_input, train_target, id_class_train)\n",
    "plot_loss(range(0, epochs_number), my_loss_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_2.forward(test_input)\n",
    "l = model_2.compute_loss(output, test_target)\n",
    "\n",
    "print(\"\\n \")\n",
    "print(\"Loss: \", l.item())\n",
    "print(\"Number of errors: \", compute_number_error(output, test_target).item())\n",
    "print(\"\\n \")\n",
    "\n",
    "\n",
    "id_class_test = output.argmax(dim=1)\n",
    "plot_result(test_input, test_target, id_class_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Dropout, constant learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.25\n",
    "\n",
    "linear_1 = Linear(2, hidden_nb)\n",
    "relu_1 = Relu()\n",
    "linear_2 = Linear(hidden_nb, hidden_nb, dropout=dropout)\n",
    "relu_2 = Relu()\n",
    "linear_3 = Linear(hidden_nb, hidden_nb, dropout=dropout)\n",
    "relu_3 = Relu()\n",
    "linear_4 = Linear(hidden_nb, 2)\n",
    "loss = CrossEntropy()\n",
    "\n",
    "model_3 = Sequential(linear_1, relu_1, linear_2, relu_2, linear_3, relu_3, linear_4, loss=CrossEntropy()) \n",
    "       \n",
    "\n",
    "my_loss_3 = []\n",
    "\n",
    "model_3.normalize_parameters(mean=0, std=std)\n",
    "optimizer = Sgd()\n",
    "\n",
    "for epochs in range(0, epochs_number):\n",
    "    for b in range(0, Sample_number, batch_size):\n",
    "        output = model_3.forward(train_input.narrow(0, b, batch_size))\n",
    "        loss_value =  model_3.compute_loss(output, train_target.narrow(0, b, batch_size))\n",
    "        model_3.backward()\n",
    "        \n",
    "        optimizer(model_3.sequence, eta=eta)\n",
    "    \n",
    "    if epochs%50 == 0:\n",
    "        print(epochs, \": \", loss_value.item())\n",
    "    my_loss_3.append(loss_value.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_3.forward(train_input)\n",
    "l = model_3.compute_loss(output, train_target)\n",
    "\n",
    "print(\"\\n \")\n",
    "print(\"Loss: \", l.item())\n",
    "print(\"Number of errors: \", compute_number_error(output, train_target).item())\n",
    "print(\"\\n \")\n",
    "\n",
    "\n",
    "id_class_train = output.argmax(dim=1)\n",
    "plot_result(train_input, train_target, id_class_train)\n",
    "plot_loss(range(0, epochs_number), my_loss_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_3.forward(test_input)\n",
    "l = model_3.compute_loss(output, test_target)\n",
    "\n",
    "print(\"\\n \")\n",
    "print(\"Loss: \", l.item())\n",
    "print(\"Number of errors: \", compute_number_error(output, test_target).item())\n",
    "print(\"\\n \")\n",
    "\n",
    "\n",
    "id_class_test = output.argmax(dim=1)\n",
    "plot_result(test_input, test_target, id_class_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: Same as 1 with tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "linear_1 = Linear(2, hidden_nb)\n",
    "relu_1 = Tanh()\n",
    "linear_2 = Linear(hidden_nb, hidden_nb)\n",
    "relu_2 = Tanh()\n",
    "linear_3 = Linear(hidden_nb, hidden_nb)\n",
    "relu_3 = Tanh()\n",
    "linear_4 = Linear(hidden_nb, 2)\n",
    "loss = CrossEntropy()\n",
    "\n",
    "model_4 = Sequential(linear_1, relu_1, linear_2, relu_2, linear_3, relu_3, linear_4, loss=CrossEntropy()) \n",
    "       \n",
    "\n",
    "my_loss_4 = []\n",
    "\n",
    "model_4.normalize_parameters(mean=0, std=std)\n",
    "optimizer = Sgd()\n",
    "\n",
    "for epochs in range(0, epochs_number):\n",
    "    for b in range(0, Sample_number, batch_size):\n",
    "        output = model_4.forward(train_input.narrow(0, b, batch_size))\n",
    "        loss_value =  model_4.compute_loss(output, train_target.narrow(0, b, batch_size))\n",
    "        model_4.backward()\n",
    "        \n",
    "        optimizer(model_4.sequence, eta=eta)\n",
    "    \n",
    "    if epochs%50 == 0:\n",
    "        print(epochs, \": \", loss_value.item())\n",
    "    my_loss_4.append(loss_value.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_4.forward(train_input)\n",
    "l = model_4.compute_loss(output, train_target)\n",
    "\n",
    "print(\"\\n \")\n",
    "print(\"Loss: \", l.item())\n",
    "print(\"Number of errors: \", compute_number_error(output, train_target).item())\n",
    "print(\"\\n \")\n",
    "\n",
    "\n",
    "id_class_train = output.argmax(dim=1)\n",
    "plot_result(train_input, train_target, id_class_train)\n",
    "plot_loss(range(0, epochs_number), my_loss_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_4.forward(test_input)\n",
    "l = model_4.compute_loss(output, test_target)\n",
    "\n",
    "print(\"\\n \")\n",
    "print(\"Loss: \", l.item())\n",
    "print(\"Number of errors: \", compute_number_error(output, test_target).item())\n",
    "print(\"\\n \")\n",
    "\n",
    "\n",
    "id_class_test = output.argmax(dim=1)\n",
    "plot_result(test_input, test_target, id_class_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4.5 : Same as 1 with tanh and xavier initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "linear_1 = Linear(2, hidden_nb)\n",
    "relu_1 = Tanh()\n",
    "linear_2 = Linear(hidden_nb, hidden_nb)\n",
    "relu_2 = Tanh()\n",
    "linear_3 = Linear(hidden_nb, hidden_nb)\n",
    "relu_3 = Tanh()\n",
    "linear_4 = Linear(hidden_nb, 2)\n",
    "loss = CrossEntropy()\n",
    "\n",
    "model_45 = Sequential(linear_1, relu_1, linear_2, relu_2, linear_3, relu_3, linear_4, loss=CrossEntropy()) \n",
    "       \n",
    "\n",
    "my_loss_45 = []\n",
    "\n",
    "model_45.xavier_parameters()\n",
    "optimizer = Sgd()\n",
    "\n",
    "\n",
    "for epochs in range(0, epochs_number):\n",
    "    for b in range(0, Sample_number, batch_size):\n",
    "        output = model_45.forward(train_input.narrow(0, b, batch_size))\n",
    "        loss_value =  model_45.compute_loss(output, train_target.narrow(0, b, batch_size))\n",
    "        model_45.backward()\n",
    "        \n",
    "        optimizer(model_45.sequence, eta=eta)\n",
    "        \n",
    "    \n",
    "    if epochs%50 == 0:\n",
    "        print(epochs, \": \", loss_value.item())\n",
    "    my_loss_45.append(loss_value.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_45.forward(train_input)\n",
    "l = model_45.compute_loss(output, train_target)\n",
    "\n",
    "print(\"\\n \")\n",
    "print(\"Loss: \", l.item())\n",
    "print(\"Number of errors: \", compute_number_error(output, train_target).item())\n",
    "print(\"\\n \")\n",
    "\n",
    "\n",
    "id_class_train = output.argmax(dim=1)\n",
    "plot_result(train_input, train_target, id_class_train)\n",
    "plot_loss(range(0, epochs_number), my_loss_45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_45.forward(test_input)\n",
    "l = model_45.compute_loss(output, test_target)\n",
    "\n",
    "print(\"\\n \")\n",
    "print(\"Loss: \", l.item())\n",
    "print(\"Number of errors: \", compute_number_error(output, test_target).item())\n",
    "print(\"\\n \")\n",
    "\n",
    "\n",
    "id_class_test = output.argmax(dim=1)\n",
    "plot_result(test_input, test_target, id_class_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5: Same as 1 with Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_lambda = 0.1\n",
    "\n",
    "linear_1 = Linear(2, hidden_nb)\n",
    "relu_1 = Sigmoid(p_lambda)\n",
    "linear_2 = Linear(hidden_nb, hidden_nb)\n",
    "relu_2 = Sigmoid(p_lambda)\n",
    "linear_3 = Linear(hidden_nb, hidden_nb)\n",
    "relu_3 = Sigmoid(p_lambda)\n",
    "linear_4 = Linear(hidden_nb, 2)\n",
    "loss = CrossEntropy()\n",
    "\n",
    "model_5 = Sequential(linear_1, relu_1, linear_2, relu_2, linear_3, relu_3, linear_4, loss=CrossEntropy()) \n",
    "       \n",
    "\n",
    "my_loss_5 = []\n",
    "\n",
    "model_5.normalize_parameters(mean=0, std=1)\n",
    "optimizer = Sgd()\n",
    "\n",
    "\n",
    "for epochs in range(0, epochs_number):\n",
    "    for b in range(0, Sample_number, batch_size):\n",
    "        output = model_5.forward(train_input.narrow(0, b, batch_size))\n",
    "        loss_value =  model_5.compute_loss(output, train_target.narrow(0, b, batch_size))\n",
    "        model_5.backward()\n",
    "\n",
    "        optimizer(model_5.sequence, eta=eta)\n",
    "    \n",
    "    if epochs%50 == 0:\n",
    "        print(epochs, \": \", loss_value.item())\n",
    "    my_loss_5.append(loss_value.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_5.forward(train_input)\n",
    "l = model_5.compute_loss(output, train_target)\n",
    "\n",
    "print(\"\\n \")\n",
    "print(\"Loss: \", l.item())\n",
    "print(\"Number of errors: \", compute_number_error(output, train_target).item())\n",
    "print(\"\\n \")\n",
    "\n",
    "\n",
    "id_class_train = output.argmax(dim=1)\n",
    "plot_result(train_input, train_target, id_class_train)\n",
    "plot_loss(range(0, epochs_number), my_loss_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_5.forward(test_input)\n",
    "l = model_5.compute_loss(output, test_target)\n",
    "\n",
    "print(\"\\n \")\n",
    "print(\"Loss: \", l.item())\n",
    "print(\"Number of errors: \", compute_number_error(output, test_target).item())\n",
    "print(\"\\n \")\n",
    "\n",
    "\n",
    "id_class_test = output.argmax(dim=1)\n",
    "plot_result(test_input, test_target, id_class_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 6: Same as 1 but with MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "linear_1 = Linear(2, hidden_nb)\n",
    "relu_1 = Relu()\n",
    "linear_2 = Linear(hidden_nb, hidden_nb)\n",
    "relu_2 = Relu()\n",
    "linear_3 = Linear(hidden_nb, hidden_nb)\n",
    "relu_3 = Relu()\n",
    "linear_4 = Linear(hidden_nb, 2)\n",
    "loss = LossMSE()\n",
    "\n",
    "model_6 = Sequential(linear_1, relu_1, linear_2, relu_2, linear_3, relu_3, linear_4, loss=CrossEntropy()) \n",
    "       \n",
    "\n",
    "\n",
    "my_loss_6 = []\n",
    "\n",
    "model_6.normalize_parameters(mean=0, std=std)\n",
    "optimizer = Sgd()\n",
    "\n",
    "for epochs in range(0, epochs_number):\n",
    "    for b in range(0, Sample_number, batch_size):\n",
    "        output = model_6.forward(train_input.narrow(0, b, batch_size))\n",
    "        loss_value =  model_6.compute_loss(output, train_target.narrow(0, b, batch_size))\n",
    "        model_6.backward()\n",
    "        \n",
    "        optimizer(model_6.sequence, eta=eta)\n",
    "    \n",
    "    if epochs%50 == 0:\n",
    "        print(epochs, \": \", loss_value.item())\n",
    "    my_loss_6.append(loss_value.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_6.forward(train_input)\n",
    "l = model_6.compute_loss(output, train_target)\n",
    "\n",
    "print(\"\\n \")\n",
    "print(\"Loss: \", l.item())\n",
    "print(\"Number of errors: \", compute_number_error(output, train_target).item())\n",
    "print(\"\\n \")\n",
    "\n",
    "\n",
    "id_class_train = output.argmax(dim=1)\n",
    "plot_result(train_input, train_target, id_class_train)\n",
    "plot_loss(range(0, epochs_number), my_loss_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_6.forward(test_input)\n",
    "l = model_6.compute_loss(output, test_target)\n",
    "\n",
    "print(\"\\n \")\n",
    "print(\"Loss: \", l.item())\n",
    "print(\"Number of errors: \", compute_number_error(output, test_target).item())\n",
    "print(\"\\n \")\n",
    "\n",
    "\n",
    "id_class_test = output.argmax(dim=1)\n",
    "plot_result(test_input, test_target, id_class_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(range(0, epochs_number), my_loss_1, linewidth=1)\n",
    "plt.plot(range(0, epochs_number), my_loss_2)\n",
    "plt.plot(range(0, epochs_number), my_loss_3,  linewidth=0.5)\n",
    "plt.legend([\"Base model\", \"Decreasing sgd\", \"Dropout\"])\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(0, epochs_number), my_loss_1, linewidth=0.5)\n",
    "plt.plot(range(0, epochs_number), my_loss_4, linewidth=0.5, alpha=0.8)\n",
    "plt.plot(range(0, epochs_number), my_loss_45, linewidth=0.5, alpha=0.8)\n",
    "plt.plot(range(0, epochs_number), my_loss_5,  linewidth=0.5)\n",
    "plt.legend([\"Relu\", \"tanh\", \"tanh xavier\", \"sigmoid\"])\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

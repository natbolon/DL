{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MINIPROJECT 1\n",
    "\n",
    "## Classification, weight sharing, auxiliary losses\n",
    "\n",
    "\n",
    "The objective of this project is to test different architectures to compare two digits visible in a two-channel image. It aims at showing in particular the impact of weight sharing, and of the use of an auxiliary loss to help the training of the main objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.init import xavier_uniform_, xavier_normal_\n",
    "\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dlc_practical_prologue import generate_pair_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_target, train_classes, test_input, test_target, test_classes = generate_pair_sets(1000)\n",
    "\n",
    "def normalize_data(train_input, test_input):\n",
    "    mu, std = train_input.mean(), train_input.std()\n",
    "    train_input.sub_(mu).div_(std)\n",
    "    test_input.sub_(mu).div_(std)\n",
    "    \n",
    "def to_one_hot(tensor):\n",
    "    \n",
    "    one_hot = torch.zeros((tensor.size(0), 10)).type(torch.FloatTensor)\n",
    "    one_hot[list(range(0,tensor.size(0))), tensor[:,0]] = 1\n",
    "    return one_hot\n",
    "\n",
    "def shuffle(t_input, classes, target):\n",
    "    idx = [i for i in range(t_input.size(0))]\n",
    "    random.shuffle(idx)\n",
    "    return t_input[idx,:,:,:], classes[idx, :], target[idx,:]\n",
    "\n",
    "def binarize(target):\n",
    "    target_bin = torch.zeros((target.size(0), 2))\n",
    "    target_bin[list(range(target.size(0))), train_target[:]] = 1\n",
    "    return target_bin\n",
    "\n",
    "\n",
    "train_target_bin, test_target_bin = binarize(train_target), binarize(test_target)\n",
    "train_input, train_classes, train_target_bin = shuffle(train_input, train_classes, train_target_bin)\n",
    "\n",
    "train_input = train_input.reshape((train_input.size(0)*2, 1, train_input.size(2), train_input.size(3)))\n",
    "test_input = test_input.reshape((test_input.size(0)*2, 1, test_input.size(2), test_input.size(3)))\n",
    "\n",
    "normalize_data(train_input, test_input)\n",
    "\n",
    "train_classes_one_hot = to_one_hot(train_classes.view(train_classes.size(0)*2, -1)).view(train_classes.size(0), -1)\n",
    "test_classes_one_hot  = to_one_hot(test_classes.view(test_classes.size(0)*2, -1)).view(test_classes.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of train samples of type 0: {}. \\nNumber of train samples of type 1: {}'.format(1000 - sum(train_target), sum(train_target)))\n",
    "print('Number of test samples of type 0: {}. \\nNumber of test samples of type 1: {}'.format(1000 - sum(test_target), sum(test_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 2, figsize=(5,12))\n",
    "ax = ax.flatten()\n",
    "for i in range(4):\n",
    "    test_i, test_c, test_t = train_input[i*2:i*2+2], train_classes[i*2], train_target_bin[i*2,:]\n",
    "    print(test_c, '\\t')\n",
    "    print('Target: {}'.format(test_t) )\n",
    "    \n",
    "    ax[i*2].imshow(test_i[0][0], cmap=\"gray\")\n",
    "    ax[i*2].set_title('Value: {}'.format(test_c[0].item()))\n",
    "    ax[i*2+1].imshow(test_i[1][0], cmap=\"gray\")\n",
    "    ax[i*2+1].set_title('Value: {}'.format(test_c[1].item()))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define train function and other auxiliary functions that can be necessary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training function for convolutional model\n",
    "\n",
    "def train_model(model, train_input, train_target, epochs=25, \\\n",
    "                mini_batch_size=200, lr=1e-3, criterion=None, optimizer=None, verbose=2):\n",
    "    # use MSE loss by default\n",
    "    if not criterion:\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "    # use SGD by default\n",
    "    if not optimizer:\n",
    "        optimizer = optim.SGD(model.parameters(), lr = lr)\n",
    "\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        sum_loss = 0\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            output = model(train_input.narrow(0, b, mini_batch_size))\n",
    "            loss = criterion(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            sum_loss = sum_loss + loss.item()\n",
    "            optimizer.step()\n",
    "            #for p in model.parameters():\n",
    "            #    p.data.sub_(lr * p.grad.data)\n",
    "        \n",
    "        if verbose == 0: print('Epoch: {}, loss: {:0.2f}'.format(e, sum_loss))\n",
    "        elif verbose == 1 and e%5 == 0: print(e, sum_loss)\n",
    "            \n",
    "            \n",
    "def train_model_conv(model, train_input, train_target, epochs=25, \\\n",
    "                mini_batch_size=200, lr=1e-3, criterion=None, optimizer=None, verbose=2):\n",
    "    \n",
    "    print('Training Conv Net')\n",
    "    \n",
    "    # use MSE loss by default\n",
    "    if not criterion:\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "    # use SGD by default\n",
    "    if not optimizer:\n",
    "        optimizer = optim.SGD(model.parameters(), lr = lr)\n",
    "\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        sum_loss = 0\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            output = model(train_input.narrow(0, b, mini_batch_size))\n",
    "            loss = criterion(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            sum_loss = sum_loss + loss.item()\n",
    "            optimizer.step()\n",
    "            #for p in model.parameters():\n",
    "            #    p.data.sub_(lr * p.grad.data)\n",
    "        \n",
    "        if verbose == 0: print('Epoch: {}, loss: {:0.2f}'.format(e, sum_loss))\n",
    "        elif verbose == 1 and e%5 == 0: print(e, sum_loss)\n",
    "            \n",
    "\n",
    "# Define training for fully connected model\n",
    "def train_model_fc(model, train_input, train_target, epochs=25, \\\n",
    "                mini_batch_size=200, lr=1e-3, criterion=None, optimizer=None, verbose=2):\n",
    "    print('Training Fully connected net')\n",
    "    \n",
    "    # use MSE loss by default\n",
    "    if not criterion:\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "    # use SGD by default\n",
    "    if not optimizer:\n",
    "        optimizer = optim.SGD(model.parameters(), lr = lr)\n",
    "\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        sum_loss = 0\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            output = model(train_input.narrow(0, b, mini_batch_size))\n",
    "            loss = criterion(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            sum_loss = sum_loss + loss.item()\n",
    "            optimizer.step()\n",
    "            #for p in model.parameters():\n",
    "            #    p.data.sub_(lr * p.grad.data)\n",
    "        \n",
    "        if verbose == 0: print('Epoch: {}, loss: {:0.2f}'.format(e, sum_loss))\n",
    "        elif verbose == 1 and e%5 == 0: print(e, sum_loss)\n",
    "            \n",
    "\n",
    "# Define model for composed model            \n",
    "def train_model_all(model, train_input, train_classes, train_target, epochs=25, \\\n",
    "                mini_batch_size=100, lr=1e-3, w1=1, w2=1, criterion1=None, criterion2=None, optimizer=None, verbose=2):\n",
    "    \n",
    "    print('Training Composed model')\n",
    "    \n",
    "    # use MSE loss by default\n",
    "    if not criterion1:\n",
    "        criterion1 = nn.MSELoss()\n",
    "    if not criterion2:\n",
    "        criterion2 = nn.MSELoss()\n",
    "        \n",
    "    # use SGD by default\n",
    "    if not optimizer:\n",
    "        optimizer = optim.SGD(model.parameters(), lr = lr)\n",
    "\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        sum_loss = 0\n",
    "        \n",
    "        for b in range(0, train_target.size(0), mini_batch_size):\n",
    "            output_classes, output_final = model(train_input.narrow(0, b*2, mini_batch_size*2))\n",
    "            loss1 = criterion1(output_classes, train_classes.narrow(0, b*2, mini_batch_size*2))\n",
    "            loss2 = criterion2(output_final, train_target.narrow(0, b, mini_batch_size))\n",
    "            loss = w1*loss1 + w2*loss2\n",
    "            model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            sum_loss = sum_loss + loss.item()\n",
    "            optimizer.step()\n",
    "            #for p in model.parameters():\n",
    "            #    p.data.sub_(lr * p.grad.data)\n",
    "        \n",
    "        if verbose == 0: print('Epoch: {}, loss: {:0.2f}'.format(e, sum_loss))\n",
    "        elif verbose == 1 and e%5 == 0: print(e, sum_loss)\n",
    "    \n",
    "            \n",
    "\n",
    "\n",
    "def compute_nb_errors(model, input, target, mini_batch_size=100):\n",
    "    errors = 0\n",
    "\n",
    "    for b in range(0, input.size(0), mini_batch_size):\n",
    "        output = model(input.narrow(0, b, mini_batch_size))\n",
    "        _, predicted_classes = output.data.max(1)\n",
    "        \n",
    "        for k in range(mini_batch_size):\n",
    "            if target.data[b + k, predicted_classes[k]] <= 0:\n",
    "                errors = errors + 1\n",
    "    return errors\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net_Conv(nn.Module):\n",
    "    def __init__(self, nb_hidden):\n",
    "        super(Net_Conv, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(256, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2, stride=1))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        x = F.relu(self.fc1(x.view(-1, 256)))\n",
    "        x = self.fc2(x)\n",
    "        return x    \n",
    "    \n",
    "\n",
    "class Net_Full(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_Full, self).__init__()\n",
    "        self.fc1 = nn.Linear(20, 100)\n",
    "        self.fc2 = nn.Linear(100,200)\n",
    "        self.fc3 = nn.Linear(200, 2)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "class Net_All(nn.Module):\n",
    "    def __init__(self, nb_h1, nb_h2, nb_h4, nb_h5):\n",
    "        super(Net_All, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,nb_h1, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(nb_h1, nb_h2, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(nb_h2, 32, kernel_size=1)\n",
    "        self.fc1 = nn.Linear(32*6*6, nb_h4)\n",
    "        self.fc2 = nn.Linear(nb_h4, 10)\n",
    "        self.fc3 = nn.Linear(20, nb_h5)\n",
    "        self.fc4 = nn.Linear(nb_h5,2)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2, stride=1))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=1))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.fc1(x.view(-1, (32*6*6))))\n",
    "        x_classes = F.relu(self.fc2(x))\n",
    "        x_out = F.relu(self.fc3(x_classes.view(-1, 20)))\n",
    "        x_out = self.fc4(x_out)\n",
    "        return x_classes, x_out\n",
    "    \n",
    "    \n",
    "class Net_small_all(nn.Module):\n",
    "    def __init__(self, nb_h1, nb_h2, nb_h3):\n",
    "        super(Net_small_all, self).__init__()\n",
    "        self.size_h2 = nb_h2\n",
    "        self.conv1 = nn.Conv2d(1,nb_h1, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(nb_h1, nb_h2, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(nb_h2*7*7, 10)\n",
    "        self.fc2 = nn.Linear(20, nb_h3)\n",
    "        self.fc3 = nn.Linear(nb_h3,2)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2, stride=1))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x_classes = F.relu(self.fc1(x.view(-1, (self.size_h2*7*7))))\n",
    "        x_out = F.relu(self.fc2(x_classes.view(-1, 20)))\n",
    "        x_out = self.fc3(x_out)\n",
    "        return x_classes, x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generate Conv + loss 1 + MLP + loss 2 feed with original classes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_target_bin, train_classes_one_hot = Variable(train_input), Variable(train_target_bin), Variable(train_classes_one_hot)\n",
    "test_input, test_target_bin, test_classes_one_hot = Variable(test_input), Variable(test_target_bin), Variable(test_classes_one_hot)\n",
    "print(train_input.shape, train_target_bin.shape, train_classes_one_hot.shape)\n",
    "print(test_input.shape, test_target_bin.shape, test_classes_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_error_test = []\n",
    "for k in range(10):\n",
    "    model1 = Net_Conv(200)\n",
    "    model2 = Net_Full()\n",
    "    p1 = sum([params.numel() for params in model1.parameters()])\n",
    "    p2 = sum([params.numel() for params in model2.parameters()])\n",
    "    print('Model with {} parameters'.format(p1+p2))\n",
    "    \n",
    "    train_model(model1, train_input, train_classes_one_hot, lr=1, verbose=2)\n",
    "    train_model(model2, train_classes_one_hot.view(1000,20), train_target_bin, lr=5e-1, verbose=2)\n",
    "    \n",
    "    out = model2(model1(test_input).view(1000, 20))\n",
    "    _, argm = out.max(1)\n",
    "    nb_test_errors = 1000 - (argm == test_target).sum(0)\n",
    "    nb_error_test.append(100.0*nb_test_errors/test_target.size(0))\n",
    "    \n",
    "    #nb_test_errors_n1 = compute_nb_errors(model1, test_input, test_classes_one_hot)\n",
    "    #nb_error_test += nb_test_errors_n1\n",
    "    print('test error Net {:0.2f}% {:d}/{:d}'.format((100 * nb_test_errors) / test_target.size(0),\n",
    "                                                      nb_test_errors, test_target.size(0)))\n",
    "\n",
    "print('Test error mu: {:0.2f}% '.format(sum(nb_error_test)/len(nb_error_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Generate Conv + loss 1 + MLP + loss 2 feed with predicted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_error_test = []\n",
    "for k in range(10):\n",
    "    model1 = Net_Conv(200)\n",
    "    model2 = Net_Full()\n",
    "    p1 = sum([params.numel() for params in model1.parameters()])\n",
    "    p2 = sum([params.numel() for params in model2.parameters()])\n",
    "    print('Model with {} parameters'.format(p1+p2))\n",
    "    train_model_conv(model1, train_input, train_classes_one_hot, lr=1, verbose=2)\n",
    "    out_classes = model1(train_input)\n",
    "    train_model_fc(model2, out_classes.view(1000,20), train_target_bin, lr=5e-1, verbose=2)   \n",
    "    \n",
    "    out = model2(model1(test_input).view(1000, 20))\n",
    "    _, argm = out.max(1)\n",
    "    nb_test_errors = 1000 - (argm == test_target).sum(0)\n",
    "    nb_error_test.append(100.0*nb_test_errors/test_target.size(0))\n",
    "    \n",
    "    #nb_test_errors_n1 = compute_nb_errors(model1, test_input, test_classes_one_hot)\n",
    "    #nb_error_test += nb_test_errors_n1\n",
    "    print('test error Net {:0.2f}% {:d}/{:d}'.format((100 * nb_test_errors) / test_target.size(0),\n",
    "                                                      nb_test_errors, test_target.size(0)))\n",
    "\n",
    "print('Test error mu: {:0.2f}% '.format(sum(nb_error_test)/len(nb_error_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT::\n",
    "\n",
    "THERE IS SOMETHING WRONG AS THE OUTPUT TARGET DOES NOT MATCH THE REALITY!!! (CHECK VISUALIZATION PART)\n",
    "CHECK HOW TO ITERATE OVER TARGET AND INPUTS AS DIMENSIONS ARE NOT THE SAME!!!\n",
    "\n",
    "-- UPDATE (7.04.19 - ) seems to be fixed although target and classes do not always make sense (saying 3 > 9) in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_error_test = []\n",
    "\n",
    "for k in range(10):\n",
    "    model = Net_All(16, 32, 64, 100)\n",
    "    p = sum([params.numel() for params in model.parameters()])\n",
    "    print('Model with {} parameters'.format(p))\n",
    "    train_model_all(model, train_input, train_classes_one_hot, train_target_bin, lr=0.5)\n",
    "    \n",
    "    out_class, out_target = model(test_input)\n",
    "    _, argmax_class = out_class.max(1)\n",
    "    _, argmax_target = out_target.max(1)\n",
    "    nb_test_errors = 1000 - (argmax_target == test_target).sum(0)\n",
    "    nb_error_test.append(100.0*nb_test_errors/test_target.size(0))\n",
    "    \n",
    "    #nb_test_errors_n1 = compute_nb_errors(model1, test_input, test_classes_one_hot)\n",
    "    #nb_error_test += nb_test_errors_n1\n",
    "    print('test error Net {:0.2f}% {:d}/{:d}'.format((100 * nb_test_errors) / test_target.size(0),\n",
    "                                                      nb_test_errors, test_target.size(0)))\n",
    "\n",
    "print('Test error mu: {:0.2f}% '.format(sum(nb_error_test)/len(nb_error_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 2, figsize=(5,12))\n",
    "ax = ax.flatten()\n",
    "for i in range(4):\n",
    "    r = int(1000*random.random())\n",
    "    test_i, test_c, test_t = test_input[r*2:r*2+2], test_classes[r], test_target[r]\n",
    "    out_c, out_t = model(test_i)\n",
    "    _, pred_c = out_c.max(1)\n",
    "    _, pred = out_t.max(1)\n",
    "    print('Target: {}, Output : {}'.format(test_t.item(), pred.item()) )\n",
    "    \n",
    "    ax[i*2].imshow(test_i[0][0], cmap=\"gray\")\n",
    "    ax[i*2].set_title('Value: {} Predicted: {}'.format(test_c[0].item(), pred_c[0].item()))\n",
    "    ax[i*2+1].imshow(test_i[1][0], cmap=\"gray\")\n",
    "    ax[i*2+1].set_title('Value: {} Predicted: {}'.format(test_c[1].item(), pred_c[1].item()))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_error_test = []\n",
    "\n",
    "for k in range(10):\n",
    "    model = Net_All(60, 120, 30, 100)\n",
    "    p = sum([params.numel() for params in model.parameters()])\n",
    "    print('Model with {} parameters'.format(p))\n",
    "    train_model_all(model, train_input, train_classes_one_hot, train_target_bin, lr=0.5, verbose=1)\n",
    "    \n",
    "    out_class, out_target = model(test_input)\n",
    "    _, argmax_class = out_class.max(1)\n",
    "    _, argmax_target = out_target.max(1)\n",
    "    nb_test_errors = 1000 - (argmax_target == test_target).sum(0)\n",
    "    nb_error_test.append(100.0*nb_test_errors/test_target.size(0))\n",
    "    \n",
    "    #nb_test_errors_n1 = compute_nb_errors(model1, test_input, test_classes_one_hot)\n",
    "    #nb_error_test += nb_test_errors_n1\n",
    "    print('test error Net {:0.2f}% {:d}/{:d}'.format((100 * nb_test_errors) / test_target.size(0),\n",
    "                                                      nb_test_errors, test_target.size(0)))\n",
    "\n",
    "print('Test error mu: {:0.2f}% '.format(sum(nb_error_test)/len(nb_error_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 2, figsize=(5,12))\n",
    "ax = ax.flatten()\n",
    "for i in range(4):\n",
    "    r = int(1000*random.random())\n",
    "    test_i, test_c, test_t = test_input[r*2:r*2+2], test_classes[r], test_target[r]\n",
    "    out_c, out_t = model(test_i)\n",
    "    _, pred_c = out_c.max(1)\n",
    "    _, pred = out_t.max(1)\n",
    "    print('Target: {}, Output : {}'.format(test_t.item(), pred.item()) )\n",
    "    \n",
    "    ax[i*2].imshow(test_i[0][0], cmap=\"gray\")\n",
    "    ax[i*2].set_title('Value: {} Predicted: {}'.format(test_c[0].item(), pred_c[0].item()))\n",
    "    ax[i*2+1].imshow(test_i[1][0], cmap=\"gray\")\n",
    "    ax[i*2+1].set_title('Value: {} Predicted: {}'.format(test_c[1].item(), pred_c[1].item()))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Generate whole network with intermediate loss - Small version ~70k parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        xavier_normal_(m.weight.data)\n",
    "        \n",
    "def weights_init_uniform(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model with pytorch default initialization')\n",
    "model = Net_small_all(40, 80, 50)\n",
    "for params in model.parameters():\n",
    "    print('Mean: {:0.2f}; Std: {:0.2f}'.format(params.mean().item(), params.std().item()))\n",
    "    \n",
    "print('Model with xavier uniform initialization')\n",
    "model = Net_small_all(40, 80, 50)\n",
    "model.apply(weights_init_uniform)\n",
    "for params in model.parameters():\n",
    "    print('Mean: {:0.2f}; Std: {:0.2f}'.format(params.mean().item(), params.std().item()))\n",
    "    \n",
    "print('Model with xavier normal initialization')\n",
    "model = Net_small_all(40, 80, 50)\n",
    "model.apply(weights_init_normal)\n",
    "for params in model.parameters():\n",
    "    print('Mean: {:0.2f}; Std: {:0.2f}'.format(params.mean().item(), params.std().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_error_test = []\n",
    "\n",
    "for k in range(10):\n",
    "    model = Net_small_all(40, 80, 50)\n",
    "    model.apply(weights_init_normal)\n",
    "    p = sum([params.numel() for params in model.parameters()])\n",
    "    if k == 0:\n",
    "        print('Model with {} parameters'.format(p))\n",
    "    \n",
    "    train_model_all(model, train_input, train_classes_one_hot, train_target_bin, lr=0.1, w1=2, verbose=1)\n",
    "    \n",
    "    out_class, out_target = model(test_input)\n",
    "    _, argmax_class = out_class.max(1)\n",
    "    _, argmax_target = out_target.max(1)\n",
    "    nb_test_errors = 1000 - (argmax_target == test_target).sum(0)\n",
    "    nb_error_test.append(100.0*nb_test_errors/test_target.size(0))\n",
    "    \n",
    "    #nb_test_errors_n1 = compute_nb_errors(model1, test_input, test_classes_one_hot)\n",
    "    #nb_error_test += nb_test_errors_n1\n",
    "    print('test error Net {:0.2f}% {:d}/{:d}'.format((100 * nb_test_errors) / test_target.size(0),\n",
    "                                                      nb_test_errors, test_target.size(0)))\n",
    "\n",
    "print('Test error mu: {:0.2f}% '.format(sum(nb_error_test)/len(nb_error_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for params in model.parameters():\n",
    "    print(params.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Generate MLP 2 - without aux loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Generate ConvNet 2 - without aux loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7. Generate "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MINIPROJECT 1\n",
    "\n",
    "## Classification, weight sharing, auxiliary losses\n",
    "\n",
    "\n",
    "The objective of this project is to test different architectures to compare two digits visible in a two-channel image. It aims at showing in particular the impact of weight sharing, and of the use of an auxiliary loss to help the training of the main objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.init import xavier_uniform_, xavier_normal_\n",
    "\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dlc_practical_prologue import generate_pair_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_target, train_classes, test_input, test_target, test_classes = generate_pair_sets(1000)\n",
    "\n",
    "def normalize_data(train_input, test_input):\n",
    "    mu, std = train_input.mean(), train_input.std()\n",
    "    train_input.sub_(mu).div_(std)\n",
    "    test_input.sub_(mu).div_(std)\n",
    "    \n",
    "def to_one_hot(tensor):\n",
    "    \n",
    "    one_hot = torch.zeros((tensor.size(0), 10)).type(torch.FloatTensor)\n",
    "    one_hot[list(range(0,tensor.size(0))), tensor[:,0]] = 1\n",
    "    return one_hot\n",
    "\n",
    "def shuffle(t_input, classes, target):\n",
    "    idx = [i for i in range(t_input.size(0))]\n",
    "    random.shuffle(idx)\n",
    "    return t_input[idx,:,:,:], classes[idx, :], target[idx,:]\n",
    "\n",
    "def binarize(target):\n",
    "    target_bin = torch.zeros((target.size(0), 2))\n",
    "    target_bin[list(range(target.size(0))), target[:]] = 1\n",
    "    return target_bin\n",
    "\n",
    "# convert to binary target\n",
    "train_target_bin, test_target_bin = binarize(train_target), binarize(test_target)\n",
    "\n",
    "# shuffle train set\n",
    "train_input, train_classes, train_target_bin = shuffle(train_input, train_classes, train_target_bin)\n",
    "\n",
    "# reshape \n",
    "train_input = train_input.reshape((train_input.size(0)*2, 1, train_input.size(2), train_input.size(3)))\n",
    "test_input = test_input.reshape((test_input.size(0)*2, 1, test_input.size(2), test_input.size(3)))\n",
    "\n",
    "# normalize\n",
    "normalize_data(train_input, test_input)\n",
    "\n",
    "# convert to one hot encoding\n",
    "train_classes_one_hot = to_one_hot(train_classes.view(train_classes.size(0)*2, -1))\n",
    "test_classes_one_hot  = to_one_hot(test_classes.view(test_classes.size(0)*2, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of train samples of type 0:\\t {}. \\t Number of train samples of type 1: \\t {}'.format(1000 - sum(train_target), sum(train_target)))\n",
    "print('Number of test samples of type 0:\\t {}.  \\t Number of test samples of type 1: \\t {}'.format(1000 - sum(test_target), sum(test_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_input.shape, train_classes_one_hot.shape, train_target_bin.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define train function and other auxiliary functions that can be necessary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training function for two-stage model\n",
    "\n",
    "def train_model(model, train_input, train_target, epochs=25, \\\n",
    "                mini_batch_size=100, lr=1e-3, criterion=None, optimizer=None, verbose=2):\n",
    "    # use MSE loss by default\n",
    "    if not criterion:\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "    # use SGD by default\n",
    "    if not optimizer:\n",
    "        optimizer = optim.SGD(model.parameters(), lr = lr)\n",
    "    loss_store = []\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        sum_loss = 0\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            output = model(train_input.narrow(0, b, mini_batch_size))\n",
    "            loss = criterion(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            sum_loss = sum_loss + loss.item()\n",
    "            optimizer.step()\n",
    "            #for p in model.parameters():\n",
    "            #    p.data.sub_(lr * p.grad.data)\n",
    "        loss_store.append(sum_loss)\n",
    "        if verbose == 0: print('Epoch: {}, loss: {:0.2f}'.format(e, sum_loss))\n",
    "        elif verbose == 1 and e%5 == 0: print(e, sum_loss)\n",
    "        \n",
    "    return loss_store    \n",
    "\n",
    "            \n",
    "#Â Define training function for complete model          \n",
    "def train_model_all(model, train_input, train_classes, train_target, epochs=25, \\\n",
    "                mini_batch_size=100, lr=1e-3, w1=1, w2=1, criterion1=None, criterion2=None, optimizer=None, verbose=2):\n",
    "    \n",
    "    print('Training Composed model')\n",
    "    \n",
    "    # use MSE loss by default\n",
    "    if not criterion1:\n",
    "        criterion1 = nn.MSELoss()\n",
    "    if not criterion2:\n",
    "        criterion2 = nn.MSELoss()\n",
    "        \n",
    "    # use SGD by default\n",
    "    if not optimizer:\n",
    "        optimizer = optim.SGD(model.parameters(), lr = lr)\n",
    "\n",
    "    loss_store = []\n",
    "    for e in range(epochs):\n",
    "        sum_loss = 0\n",
    "        \n",
    "        for b in range(0, train_target.size(0), mini_batch_size):\n",
    "            output_classes, output_final = model(train_input.narrow(0, b*2, mini_batch_size*2))\n",
    "            loss1 = criterion1(output_classes, train_classes.narrow(0, b*2, mini_batch_size*2))\n",
    "            loss2 = criterion2(output_final, train_target.narrow(0, b, mini_batch_size))\n",
    "            loss = w1*loss1 + w2*loss2\n",
    "            model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            sum_loss = sum_loss + loss.item()\n",
    "            optimizer.step()\n",
    "            #for p in model.parameters():\n",
    "            #    p.data.sub_(lr * p.grad.data)\n",
    "        loss_store.append(sum_loss)\n",
    "        \n",
    "        if verbose == 0: print('Epoch: {}, loss: {:0.2f}'.format(e, sum_loss))\n",
    "        elif verbose == 1 and e%5 == 0: print(e, sum_loss)\n",
    "        \n",
    "    return loss_store\n",
    "            \n",
    "\n",
    "\n",
    "def compute_nb_errors(model, input, target, mini_batch_size=100):\n",
    "    errors = 0\n",
    "\n",
    "    for b in range(0, input.size(0), mini_batch_size):\n",
    "        output = model(input.narrow(0, b, mini_batch_size))\n",
    "        _, predicted_classes = output.data.max(1)\n",
    "        \n",
    "        for k in range(mini_batch_size):\n",
    "            if target.data[b + k, predicted_classes[k]] <= 0:\n",
    "                errors = errors + 1\n",
    "    return errors\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net_Conv(nn.Module):\n",
    "    def __init__(self, nb_hidden):\n",
    "        super(Net_Conv, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(256, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2, stride=1))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        x = F.relu(self.fc1(x.view(-1, 256)))\n",
    "        x = self.fc2(x)\n",
    "        return x    \n",
    "    \n",
    "\n",
    "class Net_Full(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_Full, self).__init__()\n",
    "        self.fc1 = nn.Linear(20, 100)\n",
    "        self.fc2 = nn.Linear(100,200)\n",
    "        self.fc3 = nn.Linear(200, 2)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "class Net_All(nn.Module):\n",
    "    def __init__(self, nb_h1, nb_h2, nb_h4, nb_h5):\n",
    "        super(Net_All, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,nb_h1, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(nb_h1, nb_h2, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(nb_h2, 32, kernel_size=1)\n",
    "        self.fc1 = nn.Linear(32*6*6, nb_h4)\n",
    "        self.fc2 = nn.Linear(nb_h4, 10)\n",
    "        self.fc3 = nn.Linear(20, nb_h5)\n",
    "        self.fc4 = nn.Linear(nb_h5,2)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2, stride=1))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=1))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.fc1(x.view(-1, (32*6*6))))\n",
    "        x_classes = F.relu(self.fc2(x))\n",
    "        x_out = F.relu(self.fc3(x_classes.view(-1, 20)))\n",
    "        x_out = self.fc4(x_out)\n",
    "        return x_classes, x_out\n",
    "    \n",
    "    \n",
    "class Net_small_all(nn.Module):\n",
    "    def __init__(self, nb_h1, nb_h2, nb_h3):\n",
    "        super(Net_small_all, self).__init__()\n",
    "        self.size_h2 = nb_h2\n",
    "        self.conv1 = nn.Conv2d(1,nb_h1, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(nb_h1, nb_h2, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(nb_h2*7*7, 10)\n",
    "        self.fc2 = nn.Linear(20, nb_h3)\n",
    "        self.fc3 = nn.Linear(nb_h3,2)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2, stride=1))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x_classes = F.relu(self.fc1(x.view(-1, (self.size_h2*7*7))))\n",
    "        x_out = F.relu(self.fc2(x_classes.view(-1, 20)))\n",
    "        x_out = self.fc3(x_out)\n",
    "        return x_classes, x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generate Conv + loss 1 + MLP + loss 2 feed with original classes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_target_bin, train_classes_one_hot = Variable(train_input), Variable(train_target_bin), Variable(train_classes_one_hot)\n",
    "test_input, test_target_bin, test_classes_one_hot = Variable(test_input), Variable(test_target_bin), Variable(test_classes_one_hot)\n",
    "print(train_input.shape, train_target_bin.shape, train_classes_one_hot.shape)\n",
    "print(test_input.shape, test_target_bin.shape, test_classes_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_separate(nb_h, train_i_org, train_c_org, train_t_org, test_i_org=None, test_c_org=None, test_t_org=None, runs=10, epochs=25):\n",
    "    \n",
    "    # shuffle train\n",
    "    train_i_org, train_c_org, train_t_org = shuffle(train_i_org.view(train_i_org.size(0)//2, 2, train_i_org.size(2), -1), train_c_org.view(train_c_org.size(0)//2, -1), train_t_org)\n",
    "    idx = list(range(train_i_org.size(0)))\n",
    "    max_l = train_i_org.size(0)\n",
    "    \n",
    "    nb_error_test = []\n",
    "    loss_store = []\n",
    "    \n",
    "    for k in range(runs):\n",
    "        model1 = Net_Conv(nb_h)\n",
    "        model2 = Net_Full()\n",
    "\n",
    "        \n",
    "        #Â if validation mode for hyperparameter tunning\n",
    "        if test_i_org is None:\n",
    "            test_idx = idx[max_l//10*k: max_l//10*(k+1)]\n",
    "            train_idx =  [i for i in idx if i not in set(test_idx)]\n",
    "            train_i, test_i = train_i_org[train_idx, :, :, :], train_i_org[test_idx, :, :, :]\n",
    "            train_c, test_x = train_c_org[train_idx, :], train_c_org[test_idx, :]\n",
    "            train_t, test_t = train_t_org[train_idx, :], train_t_org[test_idx, :]\n",
    "        else:\n",
    "            test_i, train_i = test_i_org, train_i_org\n",
    "            test_c, train_c = test_c_org, train_c_org\n",
    "            test_t, train_t = test_t_org, train_t_org\n",
    "    \n",
    "        #print('Training convolutional model')\n",
    "        train_model(model1, train_i.view(-1, 1, train_i.size(2), train_i.size(3)), train_c.view(-1, 10), lr=1, verbose=2, epochs=epochs)\n",
    "        #print('Training fully connected model')\n",
    "        loss_store.append(train_model(model2, train_c.view(-1,20), train_t, lr=5e-1, verbose=2)) \n",
    "\n",
    "        _, out1 = model1(test_i.view(-1,1,test_i.size(2), test_i.size(3))).max(1)\n",
    "        out = model2(to_one_hot(out1.view(-1,1)).view(-1,20))\n",
    "        _, argm = out.max(1)\n",
    "        _, argm_t = test_t.max(1)\n",
    "        nb_test_errors = test_t.size(0) - ((argm == argm_t).sum(0))\n",
    "        nb_error_test.append(100.0*nb_test_errors/test_t.size(0))\n",
    "        print('test error {:0.2f}% {:d}/{:d}'.format((100.0 * nb_test_errors) / test_t.size(0),\n",
    "                                                          nb_test_errors, test_t.size(0)))\n",
    "    p1 = sum([params.numel() for params in model1.parameters()])\n",
    "    p2 = sum([params.numel() for params in model2.parameters()])\n",
    "    print('Model with {} parameters \\t'.format(p1+p2))\n",
    "    errors = torch.tensor(nb_error_test).type(torch.FloatTensor)\n",
    "    print('Mean error: {:0.2f} Std deviation in error: {:0.2f}'.format(errors.mean(), errors.std()))\n",
    "    return nb_error_test, loss_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, loss = test_model_separate(200, train_input, train_classes_one_hot, train_target_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = torch.tensor(loss)\n",
    "loss_m = l.mean(0)\n",
    "loss_std = l.std(0)\n",
    "xdata=list(range(l.size(1)))\n",
    "\n",
    "plt.figure('Final results')\n",
    "plt.title('Train Loss')\n",
    "plt.xlabel('Epochs') ; plt.ylabel('Loss')\n",
    "plt.plot(xdata, loss_m.tolist(), 'r')\n",
    "plt.fill_between(xdata, (loss_m - loss_std).tolist(), (loss_m + loss_std).tolist(),color='gray', alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors, loss = test_model_separate(200, train_input, train_classes_one_hot, train_target_bin, test_input, test_classes_one_hot, test_target_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined model. Train with 100 epochs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_joint(train_i_org, train_c_org, train_t_org, test_i_org=None, test_c_org=None, test_t_org=None, runs=10, epochs=25):\n",
    "    \n",
    "    # shuffle train\n",
    "    train_i_org, train_c_org, train_t_org = shuffle(train_i_org.view(train_i_org.size(0)//2, 2, train_i_org.size(2), -1), train_c_org.view(train_c_org.size(0)//2, -1), train_t_org)\n",
    "    idx = list(range(train_i_org.size(0)))\n",
    "    max_l = train_i_org.size(0)\n",
    "    \n",
    "    nb_error_test = []\n",
    "    loss = []\n",
    "    \n",
    "    for k in range(runs):\n",
    "        model = Net_All(16, 32, 64, 100)\n",
    "        \n",
    "        #Â if validation mode for hyperparameter tunning\n",
    "        if test_i_org is None:\n",
    "            test_idx = idx[max_l//10*k: max_l//10*(k+1)]\n",
    "            train_idx =  [i for i in idx if i not in set(test_idx)]\n",
    "            train_i, test_i = train_i_org[train_idx, :, :, :], train_i_org[test_idx, :, :, :]\n",
    "            train_c, test_x = train_c_org[train_idx, :], train_c_org[test_idx, :]\n",
    "            train_t, test_t = train_t_org[train_idx, :], train_t_org[test_idx, :]\n",
    "        else:\n",
    "            test_i, train_i = test_i_org, train_i_org\n",
    "            test_c, train_c = test_c_org, train_c_org\n",
    "            test_t, train_t = test_t_org, train_t_org\n",
    "\n",
    "        #Â train model\n",
    "        loss.append(train_model_all(model, train_i.view(-1, 1, train_i.size(2), train_i.size(3)), train_c.view(-1,10), train_t, lr=0.5, verbose=2, epochs=epochs))\n",
    "\n",
    "        # get output from test\n",
    "        out_class, out_target = model(test_i.view(-1, 1, test_i.size(2), test_i.size(3)))\n",
    "        _, argmax_class = out_class.max(1)\n",
    "        _, pred = out_target.max(1)\n",
    "        _, argm_t = test_t.max(1)\n",
    "        nb_test_errors = argm_t.size(0)  - (pred == argm_t).sum(0)\n",
    "        nb_error_test.append(100.0 * nb_test_errors/argm_t.size(0))\n",
    "        print('test error Net {:0.2f}% {:d}/{:d}'.format((100.0 * nb_test_errors) / argm_t.size(0),\n",
    "                                                          nb_test_errors, argm_t.size(0)))\n",
    "    \n",
    "    p = sum([params.numel() for params in model.parameters()])\n",
    "    print('Model with {} parameters'.format(p))\n",
    "    errors = torch.tensor(nb_error_test).type(torch.FloatTensor)\n",
    "    print('Mean error: {:0.2f} Std deviation in error: {:0.2f}'.format(errors.mean(), errors.std()))\n",
    "    return nb_error_test, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors, loss = test_model_joint(train_input, train_classes_one_hot, train_target_bin, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = torch.tensor(loss)\n",
    "loss_m = l.mean(0)\n",
    "loss_std = l.std(0)\n",
    "xdata=list(range(l.size(1)))\n",
    "\n",
    "plt.figure('Final results')\n",
    "plt.title('Train Loss')\n",
    "plt.xlabel('Epochs') ; plt.ylabel('Loss')\n",
    "plt.plot(xdata, loss_m.tolist(), 'r')\n",
    "plt.fill_between(xdata, (loss_m - loss_std).tolist(), (loss_m + loss_std).tolist(),color='gray', alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors, loss = test_model_joint(train_input, train_classes_one_hot, train_target_bin, test_input, test_classes_one_hot, test_target_bin, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = torch.tensor(loss)\n",
    "loss_m = l.mean(0)\n",
    "loss_std = l.std(0)\n",
    "xdata=list(range(l.size(1)))\n",
    "\n",
    "plt.figure('Final results')\n",
    "plt.title('Train Loss')\n",
    "plt.xlabel('Epochs') ; plt.ylabel('Loss')\n",
    "plt.plot(xdata, loss_m.tolist(), 'r')\n",
    "plt.fill_between(xdata, (loss_m - loss_std).tolist(), (loss_m + loss_std).tolist(),color='gray', alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

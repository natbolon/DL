{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miniproject 1 - Classification\n",
    "## Classification, weight sharing, auxiliary losses\n",
    "\n",
    "The objective of this project is to test different architectures to compare two digits visible in a two-channel image. It aims at showing in particular the impact of weight sharing, and of the use of an auxiliary loss to help the training of the main objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import library and define python3 as default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\" File to solve the first miniproject which is classification \"\"\"\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import dlc_practical_prologue as prologue\n",
    "\n",
    "\n",
    "__author__ = 'Eugène Lemaitre, Natalie Bolón Brun, Louis Munier'\n",
    "__version__ = '0.1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import and Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(N, normalize):\n",
    "    \"\"\"Function to import dataset from prologue\"\"\"\n",
    "    train_input, train_target, train_classes, test_input, test_target, test_classes = prologue.generate_pair_sets(N)\n",
    "\n",
    "    # Normalize data\n",
    "    if normalize:\n",
    "        mu, std = train_input.mean(), train_input.std()\n",
    "        train_input.sub_(mu).div_(std)\n",
    "        test_input.sub_(mu).div_(std)\n",
    "\n",
    "    return train_input, train_classes, train_target, test_input, test_classes, test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(tensor):\n",
    "    one_hot = torch.zeros((tensor.size(0), 10)).type(torch.FloatTensor)\n",
    "    one_hot[list(range(0,tensor.size(0))), tensor] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(train_input, train_classes, test_input, test_classes):\n",
    "    \"\"\"Split data into two set, pictures up/down.\"\"\"\n",
    "    train_input_up = Variable(train_input[:, 0, :, :].reshape(train_input.size(0), 1, train_input.size(2), train_input.size(3)))\n",
    "    train_input_down = Variable(train_input[:, 1, :, :].reshape(train_input.size(0), 1, train_input.size(2), train_input.size(3)))\n",
    "\n",
    "    train_classes_up = Variable(to_one_hot(train_classes[:,0]))\n",
    "    train_classes_down = Variable(to_one_hot(train_classes[:,1]))\n",
    "    \n",
    "    test_input_up = Variable(test_input[:, 0, :, :].reshape(test_input.size(0), 1, test_input.size(2), test_input.size(3)))\n",
    "    test_input_down = Variable(test_input[:, 1, :, :].reshape(test_input.size(0), 1, test_input.size(2), test_input.size(3)))\n",
    "    \n",
    "    test_classes_up = Variable(to_one_hot(test_classes[:, 0]))\n",
    "    test_classes_down = Variable(to_one_hot(test_classes[:, 1]))\n",
    "    \n",
    "    dict_up = {'train_input':train_input_up, 'train_classes':train_classes_up, 'test_input':test_input_up, 'test_classes':test_classes_up}\n",
    "    dict_down = {'train_input':train_input_down, 'train_classes':train_classes_down, 'test_input':test_input_down, 'test_classes':test_classes_down}\n",
    "    \n",
    "    return dict_up, dict_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_set(dict_in_up, dict_in_down, train_target, size):\n",
    "    rnd = []\n",
    "    other = np.arange(dict_in_up['train_input'].size(0))\n",
    "    \n",
    "    for i in range(size):\n",
    "        rnd.append(random.randint(0, dict_in_up['train_input'].size(0)-1))\n",
    "        np.delete(other, rnd[-1])\n",
    "        \n",
    "    dict_out_up = {'train_input':dict_in_up['train_input'][other,:,:,:], 'train_input_valid':dict_in_up['train_input'][rnd,:,:,:], \\\n",
    "                   'train_classes':dict_in_up['train_classes'][other,:], 'train_classes_valid':dict_in_up['train_classes'][rnd,:], \\\n",
    "                   'test_input':dict_in_up['test_input'], 'test_classes':dict_in_up['test_classes']}\n",
    "    \n",
    "    dict_out_down = {'train_input':dict_in_down['train_input'][other,:,:,:], 'train_input_valid':dict_in_down['train_input'][rnd,:,:,:], \\\n",
    "                     'train_classes':dict_in_down['train_classes'][other,:], 'train_classes_valid':dict_in_down['train_classes'][rnd,:], \\\n",
    "                     'test_input':dict_in_down['test_input'], 'test_classes':dict_in_down['test_classes']}\n",
    "    \n",
    "    return dict_out_up, dict_out_down, train_target[other], train_target[rnd]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define the device to work on CUDA if it is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_device(model, criterion, dict_in, train_target, test_target):\n",
    "    \"\"\"Check if cuda is available to run model on it.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        \n",
    "    print('\\nDevice : {}'.format(device))\n",
    "\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "    dict_in['train_input'], dict_in['train_classes'] = dict_in['train_input'].to(device), dict_in['train_classes'].to(device)\n",
    "    dict_in['test_input'], dict_in['test_classes'] = dict_in['test_input'].to(device), dict_in['test_classes'].to(device)\n",
    "    train_target, test_target = train_target.to(device), test_target.to(device)\n",
    "    \n",
    "    return model, criterion, dict_in, train_target, test_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_recognition(nn.Module):\n",
    "    \"\"\"Recognition model definition.\"\"\"\n",
    "    def __init__(self, nb_hidden):\n",
    "        super(Net_recognition, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(256, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2, stride=1))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        x = F.relu(self.fc1(x.view(-1, 256)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_compare(nn.Module):\n",
    "    \"\"\"Comparison model definition.\"\"\"\n",
    "    def __init__(self, nb_hidden):\n",
    "        super(Net_compare, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(4, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool1d(self.conv1(x), kernel_size=3, stride=1))\n",
    "        x = F.relu(self.fc1(x.view(-1, 4)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_recognition(model, criterion, dict_in, epochs, mini_batch_size = 100, lr = 1e-1, verbose = 2):\n",
    "    \"\"\"Training recognition model.\"\"\"\n",
    "    eta = 1e-1\n",
    "    nb_errors = []\n",
    "    output_to_train = torch.zeros(dict_in['train_classes'].size(), dtype = torch.double)\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        sum_loss = 0\n",
    "        \n",
    "        for b in range(0, dict_in['train_input'].size(0), mini_batch_size):\n",
    "            output = model(dict_in['train_input'].narrow(0, b, mini_batch_size))\n",
    "            loss = criterion(output, dict_in['train_classes'].narrow(0, b, mini_batch_size))\n",
    "            \n",
    "            if e == epochs-1:\n",
    "                output_to_train[b:b+mini_batch_size, :] = output\n",
    "\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            sum_loss += loss.item()\n",
    "\n",
    "            for p in model.parameters():\n",
    "                p.data.sub_(eta * p.grad.data)\n",
    "\n",
    "        if e < epochs-1:\n",
    "            end_print = '\\t\\t\\r'\n",
    "        else:\n",
    "            end_print = '\\n'\n",
    "                \n",
    "        if verbose == 0: print('Epoch: {}, loss: {:0.2f}'.format(e+1, sum_loss), end = end_print)\n",
    "        elif verbose == 1 and e%5 == 0: print(e+1, sum_loss)\n",
    "        \n",
    "        nb_errors.append(compute_nb_errors_recognition(model, dict_in, mini_batch_size))\n",
    "    \n",
    "    return nb_errors, output_to_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_compare(model, criterion, input_data, target, epochs, mini_batch_size = 100, lr = 1e-1, verbose = 2):\n",
    "    \"\"\"Training comparison model.\"\"\"\n",
    "    eta = 1e-1\n",
    "    nb_errors = []\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        sum_loss = 0\n",
    "        \n",
    "        for b in range(0, input_data.size(0), mini_batch_size):\n",
    "            output = model(input_data.narrow(0, b, mini_batch_size))\n",
    "            loss = criterion(output, input_data.narrow(0, b, mini_batch_size))\n",
    "\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            sum_loss += loss.item()\n",
    "\n",
    "            for p in model.parameters():\n",
    "                p.data.sub_(eta * p.grad.data)\n",
    "\n",
    "        if e < epochs-1:\n",
    "            end_print = '\\t\\t\\r'\n",
    "        else:\n",
    "            end_print = '\\n'\n",
    "                \n",
    "        if verbose == 0: print('Epoch: {}, loss: {:0.2f}'.format(e+1, sum_loss), end = end_print)\n",
    "        elif verbose == 1 and e%5 == 0: print(e+1, sum_loss)\n",
    "        \n",
    "        nb_errors.append(compute_nb_errors_compare(model, input_data, target, mini_batch_size))\n",
    "    \n",
    "    return nb_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Compute number of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors_recognition(model, dict_in, mini_batch_size = 100):\n",
    "    errors = 0\n",
    "\n",
    "    for b in range(0, dict_in['train_input_valid'].size(0), mini_batch_size):\n",
    "        output = model(dict_in['train_input_valid'].narrow(0, b, mini_batch_size))\n",
    "        _, predicted = output.data.max(1)\n",
    "\n",
    "        for k in range(mini_batch_size):\n",
    "            if dict_in['train_classes_valid'].data[b + k, predicted[k]] <= 0:\n",
    "                errors = errors + 1\n",
    "    \n",
    "    return errors*100/dict_in['train_classes_valid'].size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors_compare(model, input_data, target, mini_batch_size = 100):\n",
    "    errors = 0\n",
    "\n",
    "    for b in range(0, input_data.size(0), mini_batch_size):\n",
    "        output = model(input_data.narrow(0, b, mini_batch_size))\n",
    "        _, predicted = output.data.max(1)\n",
    "\n",
    "        for k in range(mini_batch_size):\n",
    "            if target.data[b + k, predicted[k]] <= 0:\n",
    "                errors = errors + 1\n",
    "    \n",
    "    return errors*100/target.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Adapt the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_learning_rate(learning_rate, loss, e):\n",
    "    \"\"\"Adapt the leaning rate: divide by two if oscillations are seen, multiply by two if a plateau is reached\n",
    "\n",
    "    e: epochs\n",
    "    \"\"\"\n",
    "    lst = loss[e - 50:e]\n",
    "    \n",
    "    if e > 50:\n",
    "        # Decreases learning rate if high variation in the loss\n",
    "        if loss[e] - loss[e - 1] - 0.5 > 0 and e > 5:\n",
    "            learning_rate = learning_rate/2\n",
    "            #optimizer = optim.SGD(model.parameters(), lr = learning_rate)\n",
    "            #print('Decrease learning rate ',learning_rate)\n",
    "\n",
    "        # Increases learning rate if a plateau is reached\n",
    "        elif abs(sum(lst)/len(lst) - loss[e]) < 0.05:\n",
    "            learning_rate = 2*learning_rate\n",
    "            #optimizer = optim.SGD(model.parameters(), lr = learning_rate)\n",
    "            #print('Increase learning rate ', learning_rate)\n",
    "            \n",
    "    return learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Device : cpu\n",
      "Epoch: 25, loss: 0.32\t\t\n",
      "Epoch: 25, loss: 0.23\t\t\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Double but got scalar type Float for argument #2 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-66c3b580bedf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mto_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mnb_final_tests\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_compare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDoubleTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdyn_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Dynamically adapt learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-437bfa0e1e4a>\u001b[0m in \u001b[0;36mtrain_model_compare\u001b[0;34m(model, criterion, input_data, target, epochs, mini_batch_size, lr, verbose)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-704b1e54ba9a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 187\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Double but got scalar type Float for argument #2 'weight'"
     ]
    }
   ],
   "source": [
    "## Define variables\n",
    "N, normalize = 1000, True\n",
    "hidden_layer = 200\n",
    "repeat, validation_size = 5, 200\n",
    "dict_up_valid = {}\n",
    "epochs, mini_batch_size, dyn_lr, verbose = 25, 100, 0.5, 0\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Import data\n",
    "train_input, train_classes, train_target, test_input, test_classes, test_target = import_data(N, normalize)\n",
    "\n",
    "# Main process\n",
    "loss = []\n",
    "xdata = np.arange(2*epochs)\n",
    "nb_error_test = np.empty([repeat, 2*epochs])\n",
    "nb_final_tests = np.empty([repeat, epochs])\n",
    "\n",
    "for r in range(repeat):\n",
    "    # Define models\n",
    "    model_recognition = Net_recognition(hidden_layer)\n",
    "    model_compare = Net_compare(hidden_layer)\n",
    "    \n",
    "    # Process data\n",
    "    dict_up, dict_down = split_data(train_input, train_classes, test_input, test_classes)\n",
    "    model_recognition, criterion, dict_up, train_target, test_target = \\\n",
    "        define_device(model_recognition, criterion, dict_up, train_target, test_target)\n",
    "    \n",
    "    # Define validation set\n",
    "    dict_up_valid, dict_down_valid, new_train_target, train_target_valid = \\\n",
    "        validation_set(dict_up, dict_down, train_target, validation_size)\n",
    "    \n",
    "    to_train = torch.zeros([dict_up_valid['train_classes'].size(0), 2*dict_up_valid['train_classes'].size(1)]).type(torch.DoubleTensor)\n",
    "\n",
    "    # Training models\n",
    "    nb_error_test[r, :epochs], to_train[:,:dict_up_valid['train_classes'].size(1)] = train_model_recognition(model_recognition, criterion, dict_up_valid, epochs, mini_batch_size, lr = dyn_lr, verbose = verbose)\n",
    "    nb_error_test[r, epochs:], to_train[:,dict_up_valid['train_classes'].size(1):] = train_model_recognition(model_recognition, criterion, dict_down_valid, epochs, mini_batch_size, lr = dyn_lr, verbose = verbose)\n",
    "    \n",
    "    to_train = to_train.reshape(to_train.size(0), 1, to_train.size(1))\n",
    "    nb_final_tests[r, :] = train_model_compare(model_compare, criterion, to_train.type(torch.DoubleTensor), train_target_valid, epochs, mini_batch_size, lr = dyn_lr, verbose = verbose)\n",
    "    \n",
    "    # Dynamically adapt learning rate\n",
    "    #dyn_lr = adapt_learning_rate(dyn_lr, loss, e)    \n",
    "\n",
    "plt.figure('Final results')\n",
    "plt.title('Final results')\n",
    "plt.xlabel('Epochs [-]') ; plt.ylabel('Errors [%]')\n",
    "plt.plot(xdata, nb_error_test.mean(0), 'r')\n",
    "plt.fill_between(xdata, nb_error_test.mean(0) - nb_error_test.std(0), nb_error_test.mean(0) + nb_error_test.std(0),color='gray', alpha=0.2)\n",
    "print(\"\\n\\nLast mean error : {}%\".format(nb_error_test[:, -1].mean(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

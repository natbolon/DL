{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miniproject 1 - Classification\n",
    "## Classification, weight sharing, auxiliary losses\n",
    "\n",
    "The objective of this project is to test different architectures to compare two digits visible in a two-channel image. It aims at showing in particular the impact of weight sharing, and of the use of an auxiliary loss to help the training of the main objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import library and define python3 as default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\" File to solve the first miniproject which is classification \"\"\"\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import dlc_practical_prologue as prologue\n",
    "\n",
    "\n",
    "__author__ = 'Eugène Lemaitre, Natalie Bolón Brun, Louis Munier'\n",
    "__version__ = '0.1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import and Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(N, normalize):\n",
    "    \"\"\"Function to import dataset from prologue\"\"\"\n",
    "    train_input, train_target, train_classes, test_input, test_target, test_classes = prologue.generate_pair_sets(N)\n",
    "\n",
    "    # Normalize data\n",
    "    if normalize:\n",
    "        mu, std = train_input.mean(), train_input.std()\n",
    "        train_input.sub_(mu).div_(std)\n",
    "        test_input.sub_(mu).div_(std)\n",
    "\n",
    "    return train_input, train_classes, train_target, test_input, test_classes, test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(tensor):\n",
    "    one_hot = torch.zeros((tensor.size(0), 10)).type(torch.FloatTensor)\n",
    "    one_hot[list(range(0,tensor.size(0))), tensor] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_validation(data, device, size=200):\n",
    "    \"\"\"Function to split the data into up and down categories with also validation set.\"\"\"\n",
    "    rnd = []\n",
    "    up, down = 0, 1\n",
    "    valid_idx = []\n",
    "    train_idx = list(range(0, data.input.size(0)))\n",
    "    \n",
    "    for i in range(size):\n",
    "        rnd = random.randint(0, len(train_idx)-1)\n",
    "        valid_idx.append(train_idx.pop(rnd))\n",
    "    \n",
    "    # Create class variables to well store all the data\n",
    "    data_up = DataStoredValid(data.input[train_idx, up, :, :].reshape(len(train_idx), 1, data.input.size(2), data.input.size(3)).to(device),\n",
    "                              to_one_hot(data.classes[train_idx, up]).to(device),\n",
    "                              data.target[train_idx].to(device),\n",
    "                              data.input[valid_idx, up, :, :].reshape(len(valid_idx), 1, data.input.size(2), data.input.size(3)).to(device),\n",
    "                              to_one_hot(data.classes[valid_idx, up]).to(device),\n",
    "                              data.target[valid_idx].to(device))\n",
    "    \n",
    "    data_down = DataStoredValid(data.input[train_idx, down, :, :].reshape(len(train_idx), 1, data.input.size(2), data.input.size(3)).to(device),\n",
    "                                to_one_hot(data.classes[train_idx, down]).to(device),\n",
    "                                data.target[train_idx].to(device),\n",
    "                                data.input[valid_idx, down, :, :].reshape(len(valid_idx), 1, data.input.size(2), data.input.size(3)).to(device),\n",
    "                                to_one_hot(data.classes[valid_idx, down]).to(device),\n",
    "                                data.target[valid_idx].to(device))\n",
    "    \n",
    "    return data_up, data_down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Classes to well store data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataStored:\n",
    "    \"\"\"A class to well store data to have a cleaner code.\"\"\"\n",
    "    def __init__(self, data_in, classes, target):\n",
    "        self.input = data_in\n",
    "        self.classes = classes\n",
    "        self.target = target\n",
    "\n",
    "\n",
    "class DataStoredValid(DataStored):\n",
    "    \"\"\"\"A class to well store data with validation set to have a cleaner code.\"\"\"\n",
    "    def __init__(self, data_in, classes, target, valid_input, valid_classes, valid_target):\n",
    "        DataStored.__init__(self, data_in, classes, target)\n",
    "        self.valid_input = valid_input\n",
    "        self.valid_classes = valid_classes\n",
    "        self.valid_target = valid_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define the device to work on CUDA if it is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_device():\n",
    "    \"\"\"Check if cuda is available to run model on it.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        \n",
    "    print('\\nDevice : {}'.format(device))\n",
    "    \n",
    "    return device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_recognition(nn.Module):\n",
    "    \"\"\"Recognition model definition.\"\"\"\n",
    "    def __init__(self, nb_hidden):\n",
    "        super(Net_recognition, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(256, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2, stride=1))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        x = F.relu(self.fc1(x.view(-1, 256)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_compare(nn.Module):\n",
    "    \"\"\"Comparison model definition.\"\"\"\n",
    "    def __init__(self, nb_hidden):\n",
    "        super(Net_compare, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(4, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool1d(self.conv1(x), kernel_size=3, stride=1))\n",
    "        x = F.relu(self.fc1(x.view(-1, 4)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Define training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_recognition(model, criterion, data, e,\n",
    "                            epochs = 25, mini_batch_size = 100, lr = 1e-1, verbose = 2):\n",
    "    \"\"\"Training recognition model.\"\"\"\n",
    "    eta = 1e-1\n",
    "    output_to_train = torch.zeros(data.classes.size()).type(torch.DoubleTensor)\n",
    "    \n",
    "    sum_loss = 0\n",
    "\n",
    "    for b in range(0, data.input.size(0), mini_batch_size):\n",
    "        output = model(data.input.narrow(0, b, mini_batch_size))\n",
    "        loss = criterion(output, data.classes.narrow(0, b, mini_batch_size))\n",
    "        \n",
    "        output_to_train[b:b+mini_batch_size, :] = output\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        sum_loss += loss.item()\n",
    "\n",
    "        for p in model.parameters():\n",
    "            p.data.sub_(eta * p.grad.data)\n",
    "\n",
    "    if verbose == 0: print('Epoch: {}, loss: {:0.2f}'.format(e+1, sum_loss), end = '\\t\\t\\r')\n",
    "    elif verbose == 1 and e%5 == 0: print(e+1, sum_loss)\n",
    "\n",
    "    nb_errors = compute_nb_errors(model, data, 'classes', mini_batch_size)\n",
    "    return nb_errors, output_to_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_compare(model, criterion, train, data, e,\n",
    "                        epochs = 25, mini_batch_size = 100, lr = 1e-1, verbose = 2):\n",
    "    \"\"\"Training comparison model.\"\"\"\n",
    "    eta = 1e-1\n",
    "    \n",
    "    sum_loss = 0\n",
    "\n",
    "    for b in range(0, train.size(0), mini_batch_size):\n",
    "        output = model(train.narrow(0, b, mini_batch_size))\n",
    "        loss = criterion(output, data.target.narrow(0, b, mini_batch_size))\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        sum_loss += loss.item()\n",
    "\n",
    "        for p in model.parameters():\n",
    "            p.data.sub_(eta * p.grad.data)\n",
    "\n",
    "    if verbose == 0: print('Epoch: {}, loss: {:0.2f}'.format(e+1, sum_loss), end = '\\t\\t\\r')\n",
    "    elif verbose == 1 and e%5 == 0: print(e+1, sum_loss)\n",
    "\n",
    "    nb_errors = compute_nb_errors(model, data, 'target', train, mini_batch_size)\n",
    "    return nb_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_architecture(data, device, criterion, validation_size, nb_mid_error, nb_final_error,\n",
    "                       repeat, r, epochs = 25, lr = 1e-1, verbose = 2):\n",
    "    print('Run : {}'.format(r+1))\n",
    "    \n",
    "    # Define models\n",
    "    model_recognition = Net_recognition(hidden_layer).to(device)\n",
    "    model_compare = Net_compare(hidden_layer).to(device)\n",
    "    \n",
    "    # Process data\n",
    "    data_up, data_down = split_data_validation(data, device, size=validation_size)\n",
    "    output_to_train = torch.zeros([data_up.classes.size(0), 1, 2*data_up.classes.size(1)]).type(torch.DoubleTensor)\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        # Training models\n",
    "        nb_mid_error[e, r], output_to_train[:, 0, :data_up.classes.size(1)] = train_model_recognition(model_recognition, criterion, data_up, e, verbose=verbose)\n",
    "        nb_mid_error[e + epochs, r], output_to_train[:, 0, data_up.classes.size(1):] = train_model_recognition(model_recognition, criterion, data_down, e, verbose=verbose)\n",
    "        \n",
    "        nb_final_error = train_model_compare(model_compare, criterion, output_to_train, data_up, e)\n",
    "\n",
    "        # Dynamically adapt learning rate\n",
    "        #dyn_lr = adapt_learning_rate(dyn_lr, loss, e)\n",
    "    \n",
    "    print('\\n')\n",
    "    return nb_mid_error, nb_final_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Compute number of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, data, target, mini_batch_size = 100):\n",
    "    errors = 0\n",
    "\n",
    "    for b in range(0, data.valid_input.size(0), mini_batch_size):\n",
    "        if target is 'classes' :\n",
    "            output = model(data.valid_input.narrow(0, b, mini_batch_size))\n",
    "        elif target is 'target' :\n",
    "            output = model(data.valid_classes.narrow(0, b, mini_batch_size))\n",
    "            \n",
    "        _, predicted = output.data.max(1)\n",
    "\n",
    "        if target is 'classes' :\n",
    "            for k in range(mini_batch_size):\n",
    "                if data.valid_classes.data[b + k, predicted[k]] <= 0:\n",
    "                    errors = errors + 1\n",
    "        elif target is 'target' :\n",
    "            for k in range(mini_batch_size):\n",
    "                if data.valid_target.data[b + k, predicted[k]] <= 0:\n",
    "                    errors = errors + 1\n",
    "    \n",
    "    return errors*100/data.valid_classes.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Usefull functions to improve learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_learning_rate(learning_rate, loss, e):\n",
    "    \"\"\"Adapt the leaning rate: divide by two if oscillations are seen, multiply by two if a plateau is reached\n",
    "\n",
    "    e: epochs\n",
    "    \"\"\"\n",
    "    lst = loss[e - 50:e]\n",
    "    \n",
    "    if e > 50:\n",
    "        # Decreases learning rate if high variation in the loss\n",
    "        if loss[e] - loss[e - 1] - 0.5 > 0 and e > 5:\n",
    "            learning_rate = learning_rate/2\n",
    "            #optimizer = optim.SGD(model.parameters(), lr = learning_rate)\n",
    "            #print('Decrease learning rate ',learning_rate)\n",
    "\n",
    "        # Increases learning rate if a plateau is reached\n",
    "        elif abs(sum(lst)/len(lst) - loss[e]) < 0.05:\n",
    "            learning_rate = 2*learning_rate\n",
    "            #optimizer = optim.SGD(model.parameters(), lr = learning_rate)\n",
    "            #print('Increase learning rate ', learning_rate)\n",
    "            \n",
    "    return learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(errors, str_results):\n",
    "    dim = 1\n",
    "    xdata = np.arange(1, np.shape(errors)[0]+1)\n",
    "\n",
    "    plt.figure(str_results)\n",
    "    plt.title(str_results)\n",
    "    plt.xlabel('Epochs [-]')\n",
    "    plt.ylabel('Errors [%]')\n",
    "\n",
    "    plt.plot(xdata, errors.mean(dim), 'r')\n",
    "    plt.fill_between(xdata,\n",
    "                     errors.mean(dim) - errors.std(dim),\n",
    "                     errors.mean(dim) + errors.std(dim),\n",
    "                     color='gray',\n",
    "                     alpha=0.2)\n",
    "\n",
    "    print(str_results)\n",
    "    print(\"\\n\\nLast mean error : {}%\".format(errors[-1].mean()))\n",
    "    print(\"Last standard deviation : {}\".format(errors[-1].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Device : cpu\n",
      "Run : 1\n",
      "Epoch: 1, loss: 0.65\t\t\r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Double but got scalar type Float for argument #2 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-8cece2f909ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     nb_mid_error, nb_final_error = train_architecture(train_original, device, criterion, validation_size, nb_mid_error,\n\u001b[0;32m---> 21\u001b[0;31m                                                       nb_final_error, repeat, r, epochs=epochs, verbose=verbose)\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mplot_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_mid_error\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Intermediate results up'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-79-fee041f2b1fa>\u001b[0m in \u001b[0;36mtrain_architecture\u001b[0;34m(data, device, criterion, validation_size, nb_mid_error, nb_final_error, repeat, r, epochs, lr, verbose)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mnb_mid_error\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_to_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_up\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model_recognition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_recognition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_down\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mnb_final_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_compare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_to_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_up\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Dynamically adapt learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-63f1ca79ff39>\u001b[0m in \u001b[0;36mtrain_model_compare\u001b[0;34m(model, criterion, train, data, e, epochs, mini_batch_size, lr, verbose)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-704b1e54ba9a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 187\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Double but got scalar type Float for argument #2 'weight'"
     ]
    }
   ],
   "source": [
    "## Define variables\n",
    "N, normalize = 1000, True\n",
    "hidden_layer = 200\n",
    "repeat, validation_size = 5, 200\n",
    "\n",
    "epochs, mini_batch_size, dyn_lr, verbose = 25, 100, 1e-1, 0\n",
    "device = define_device()\n",
    "criterion = nn.MSELoss().to(device)\n",
    "\n",
    "# Import data\n",
    "train_input, train_classes, train_target, test_input, test_classes, test_target = import_data(N, normalize)\n",
    "train_original = DataStored(train_input, train_classes, train_target)\n",
    "test_original = DataStored(test_input, test_classes, test_target)\n",
    "\n",
    "# Main process\n",
    "nb_mid_error = np.zeros((2*epochs, repeat))\n",
    "nb_final_error = np.zeros((epochs, repeat))\n",
    "\n",
    "for r in range(repeat):\n",
    "    nb_mid_error, nb_final_error = train_architecture(train_original, device, criterion, validation_size, nb_mid_error,\n",
    "                                                      nb_final_error, repeat, r, epochs=epochs, verbose=verbose)\n",
    "\n",
    "plot_results(nb_mid_error[:epochs, :], 'Intermediate results up')\n",
    "plot_results(nb_mid_error[epochs:, :], 'Intermediate results down')\n",
    "#plot_results(nb_final_error, 'Final results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

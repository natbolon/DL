{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miniproject 1 - Classification\n",
    "## Classification, weight sharing, auxiliary losses\n",
    "\n",
    "The objective of this project is to test different architectures to compare two digits visible in a two-channel image. It aims at showing in particular the impact of weight sharing, and of the use of an auxiliary loss to help the training of the main objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import library and define python3 as default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\" File to solve the first miniproject which is classification \"\"\"\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import dlc_practical_prologue as prologue\n",
    "\n",
    "\n",
    "__author__ = 'Eugène Lemaitre, Natalie Bolón Brun, Louis Munier'\n",
    "__version__ = '0.1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import and Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(N, normalize):\n",
    "    \"\"\"Function to import dataset from prologue\"\"\"\n",
    "    train_input, train_target, train_classes, test_input, test_target, test_classes = prologue.generate_pair_sets(N)\n",
    "\n",
    "    # Normalize data\n",
    "    if normalize:\n",
    "        mu, std = train_input.mean(), train_input.std()\n",
    "        train_input.sub_(mu).div_(std)\n",
    "        test_input.sub_(mu).div_(std)\n",
    "\n",
    "    return train_input, train_classes, train_target, test_input, test_classes, test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(tensor):\n",
    "    one_hot = torch.zeros((tensor.size(0), 10)).type(torch.FloatTensor)\n",
    "    one_hot[list(range(0,tensor.size(0))), tensor] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_validation(data, device, size=200):\n",
    "    \"\"\"Function to split the data into up and down categories with also validation set.\"\"\"\n",
    "    rnd = []\n",
    "    up, down = 0, 1\n",
    "    valid_idx = []\n",
    "    train_idx = list(range(0, data.input.size(0)))\n",
    "    \n",
    "    for i in range(size):\n",
    "        rnd = random.randint(0, len(train_idx)-1)\n",
    "        valid_idx.append(train_idx.pop(rnd))\n",
    "    \n",
    "    # Create class variables to well store all the data\n",
    "    data_up = DataStoredValid(data.input[train_idx, up, :, :].reshape(len(train_idx), 1, data.input.size(2), data.input.size(3)).to(device),\n",
    "                              to_one_hot(data.classes[train_idx, up]).to(device),\n",
    "                              data.target[train_idx].to(device),\n",
    "                              data.input[valid_idx, up, :, :].reshape(len(valid_idx), 1, data.input.size(2), data.input.size(3)).to(device),\n",
    "                              to_one_hot(data.classes[valid_idx, up]).to(device),\n",
    "                              data.target[valid_idx].to(device))\n",
    "    \n",
    "    data_down = DataStoredValid(data.input[train_idx, down, :, :].reshape(len(train_idx), 1, data.input.size(2), data.input.size(3)).to(device),\n",
    "                                to_one_hot(data.classes[train_idx, down]).to(device),\n",
    "                                data.target[train_idx].to(device),\n",
    "                                data.input[valid_idx, down, :, :].reshape(len(valid_idx), 1, data.input.size(2), data.input.size(3)).to(device),\n",
    "                                to_one_hot(data.classes[valid_idx, down]).to(device),\n",
    "                                data.target[valid_idx].to(device))\n",
    "    \n",
    "    return data_up, data_down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Class to well store data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataStored:\n",
    "    \"\"\"A class to well store data to have a cleaner code.\"\"\"\n",
    "    def __init__(self, data_in, classes, target):\n",
    "        self.input = data_in\n",
    "        self.classes = classes\n",
    "        self.target = target\n",
    "\n",
    "\n",
    "class DataStoredValid(DataStored):\n",
    "    \"\"\"\"A class to well store data with validation set to have a cleaner code.\"\"\"\n",
    "    def __init__(self, data_in, classes, target, valid_input, valid_classes, valid_target):\n",
    "        DataStored.__init__(self, data_in, classes, target)\n",
    "        self.valid_input = valid_input\n",
    "        self.valid_classes = valid_classes\n",
    "        self.valid_target = valid_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define the device to work on CUDA if it is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_device():\n",
    "    \"\"\"Check if cuda is available to run model on it.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        \n",
    "    print('\\nDevice : {}'.format(device))\n",
    "    \n",
    "    return device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_recognition(nn.Module):\n",
    "    \"\"\"Recognition model definition.\"\"\"\n",
    "    def __init__(self, nb_hidden):\n",
    "        super(Net_recognition, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(256, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2, stride=1))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        x = F.relu(self.fc1(x.view(-1, 256)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_compare(nn.Module):\n",
    "    \"\"\"Comparison model definition.\"\"\"\n",
    "    def __init__(self, nb_hidden):\n",
    "        super(Net_compare, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(4, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool1d(self.conv1(x), kernel_size=3, stride=1))\n",
    "        x = F.relu(self.fc1(x.view(-1, 4)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Define training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_recognition(model, criterion, data, epochs = 25, mini_batch_size = 100, lr = 1e-1, verbose = 2):\n",
    "    \"\"\"Training recognition model.\"\"\"\n",
    "    eta = 1e-1\n",
    "    nb_errors = []\n",
    "#    output_to_train = torch.zeros(dict_in['train_classes'].size(), dtype = torch.double)\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        sum_loss = 0\n",
    "        \n",
    "        for b in range(0, data.input.size(0), mini_batch_size):\n",
    "            output = model(data.input.narrow(0, b, mini_batch_size))\n",
    "            loss = criterion(output, data.classes.narrow(0, b, mini_batch_size))\n",
    "            \n",
    "#            if e == epochs-1:\n",
    "#                output_to_train[b:b+mini_batch_size, :] = output\n",
    "\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            sum_loss += loss.item()\n",
    "\n",
    "            for p in model.parameters():\n",
    "                p.data.sub_(eta * p.grad.data)\n",
    "\n",
    "        if e < epochs-1:\n",
    "            end_print = '\\t\\t\\r'\n",
    "        else:\n",
    "            end_print = '\\n'\n",
    "                \n",
    "        if verbose == 0: print('Epoch: {}, loss: {:0.2f}'.format(e+1, sum_loss), end = end_print)\n",
    "        elif verbose == 1 and e%5 == 0: print(e+1, sum_loss)\n",
    "        \n",
    "        nb_errors.append(compute_nb_errors_recognition(model, data, mini_batch_size))\n",
    "    \n",
    "    return nb_errors#, output_to_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_compare(model, criterion, input_data, target, epochs = 25, mini_batch_size = 100, lr = 1e-1, verbose = 2):\n",
    "    \"\"\"Training comparison model.\"\"\"\n",
    "    eta = 1e-1\n",
    "    nb_errors = []\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        sum_loss = 0\n",
    "        \n",
    "        for b in range(0, input_data.size(0), mini_batch_size):\n",
    "            output = model(input_data.narrow(0, b, mini_batch_size))\n",
    "            loss = criterion(output, input_data.narrow(0, b, mini_batch_size))\n",
    "\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            sum_loss += loss.item()\n",
    "\n",
    "            for p in model.parameters():\n",
    "                p.data.sub_(eta * p.grad.data)\n",
    "\n",
    "        if e < epochs-1:\n",
    "            end_print = '\\t\\t\\r'\n",
    "        else:\n",
    "            end_print = '\\n'\n",
    "                \n",
    "        if verbose == 0: print('Epoch: {}, loss: {:0.2f}'.format(e+1, sum_loss), end = end_print)\n",
    "        elif verbose == 1 and e%5 == 0: print(e+1, sum_loss)\n",
    "        \n",
    "        nb_errors.append(compute_nb_errors_compare(model, input_data, target, mini_batch_size))\n",
    "    \n",
    "    return nb_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_architecture(data, device, criterion, validation_size, nb_error_test,\n",
    "                       repeat, r, epochs = 25, lr = 1e-1, verbose = 2):\n",
    "    print('Run : {}'.format(r+1))\n",
    "    \n",
    "    # Define models\n",
    "    model_recognition = Net_recognition(hidden_layer).to(device)\n",
    "    model_compare = Net_compare(hidden_layer).to(device)\n",
    "    \n",
    "    # Process data\n",
    "    data_up, data_down = split_data_validation(data, device, size=validation_size)\n",
    "    \n",
    "    # Training models\n",
    "    nb_error_test[r, :epochs] = train_model_recognition(model_recognition, criterion, data_up, verbose=verbose)\n",
    "    nb_error_test[r, epochs:] = train_model_recognition(model_recognition, criterion, data_down, verbose=verbose)\n",
    "    \n",
    "    # Dynamically adapt learning rate\n",
    "    #dyn_lr = adapt_learning_rate(dyn_lr, loss, e)\n",
    "    \n",
    "    return nb_error_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Compute number of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors_recognition(model, data, mini_batch_size = 100):\n",
    "    errors = 0\n",
    "\n",
    "    for b in range(0, data.valid_input.size(0), mini_batch_size):\n",
    "        output = model(data.valid_input.narrow(0, b, mini_batch_size))\n",
    "        _, predicted = output.data.max(1)\n",
    "\n",
    "        for k in range(mini_batch_size):\n",
    "            if data.valid_classes.data[b + k, predicted[k]] <= 0:\n",
    "                errors = errors + 1\n",
    "    \n",
    "    return errors*100/data.valid_classes.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors_compare(model, input_data, target, mini_batch_size = 100):\n",
    "    errors = 0\n",
    "\n",
    "    for b in range(0, input_data.size(0), mini_batch_size):\n",
    "        output = model(input_data.narrow(0, b, mini_batch_size))\n",
    "        _, predicted = output.data.max(1)\n",
    "\n",
    "        for k in range(mini_batch_size):\n",
    "            if target.data[b + k, predicted[k]] <= 0:\n",
    "                errors = errors + 1\n",
    "    \n",
    "    return errors*100/target.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Usefull functions to improve learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_learning_rate(learning_rate, loss, e):\n",
    "    \"\"\"Adapt the leaning rate: divide by two if oscillations are seen, multiply by two if a plateau is reached\n",
    "\n",
    "    e: epochs\n",
    "    \"\"\"\n",
    "    lst = loss[e - 50:e]\n",
    "    \n",
    "    if e > 50:\n",
    "        # Decreases learning rate if high variation in the loss\n",
    "        if loss[e] - loss[e - 1] - 0.5 > 0 and e > 5:\n",
    "            learning_rate = learning_rate/2\n",
    "            #optimizer = optim.SGD(model.parameters(), lr = learning_rate)\n",
    "            #print('Decrease learning rate ',learning_rate)\n",
    "\n",
    "        # Increases learning rate if a plateau is reached\n",
    "        elif abs(sum(lst)/len(lst) - loss[e]) < 0.05:\n",
    "            learning_rate = 2*learning_rate\n",
    "            #optimizer = optim.SGD(model.parameters(), lr = learning_rate)\n",
    "            #print('Increase learning rate ', learning_rate)\n",
    "            \n",
    "    return learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Device : cpu\n",
      "Run : 1\n",
      "Epoch: 25, loss: 0.32\t\t\n",
      "Epoch: 25, loss: 0.21\t\t\n",
      "Run : 2\n",
      "Epoch: 25, loss: 0.30\t\t\n",
      "Epoch: 25, loss: 0.22\t\t\n",
      "Run : 3\n",
      "Epoch: 25, loss: 0.33\t\t\n",
      "Epoch: 14, loss: 0.26\t\t\r"
     ]
    }
   ],
   "source": [
    "## Define variables\n",
    "N, normalize = 1000, True\n",
    "hidden_layer = 200\n",
    "repeat, validation_size = 5, 200\n",
    "\n",
    "epochs, mini_batch_size, dyn_lr, verbose = 25, 100, 1e-1, 0\n",
    "device = define_device()\n",
    "criterion = nn.MSELoss().to(device)\n",
    "\n",
    "# Import data\n",
    "train_input, train_classes, train_target, test_input, test_classes, test_target = import_data(N, normalize)\n",
    "train_original = DataStored(train_input, train_classes, train_target)\n",
    "test_original = DataStored(test_input, test_classes, test_target)\n",
    "\n",
    "# Main process\n",
    "xdata = np.arange(1, 2*epochs+1)\n",
    "nb_final_tests = np.empty([repeat, epochs])\n",
    "nb_error_test = np.empty([repeat, 2*epochs])\n",
    "\n",
    "for r in range(repeat):\n",
    "    nb_error_test = train_architecture(train_original, device, criterion, validation_size, nb_error_test,\n",
    "                                       repeat, r, epochs, verbose=verbose)\n",
    "    \n",
    "plt.figure('Final results')\n",
    "plt.title('Final results')\n",
    "plt.xlabel('Epochs [-]')\n",
    "plt.ylabel('Errors [%]')\n",
    "\n",
    "plt.plot(xdata, nb_error_test.mean(0), 'r')\n",
    "plt.fill_between(xdata,\n",
    "                 nb_error_test.mean(0) - nb_error_test.std(0),\n",
    "                 nb_error_test.mean(0) + nb_error_test.std(0),\n",
    "                 color='gray',\n",
    "                 alpha=0.2)\n",
    "\n",
    "print(\"\\n\\nLast mean error : {}%\".format(nb_error_test[:, -1].mean(0)))\n",
    "print(\"Last standard deviation : {}\".format(nb_error_test[:, -1].std(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

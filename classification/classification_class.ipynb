{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miniproject 1 - Classification\n",
    "## Classification, weight sharing, auxiliary losses\n",
    "\n",
    "The objective of this project is to test different architectures to compare two digits visible in a two-channel image. It aims at showing in particular the impact of weight sharing, and of the use of an auxiliary loss to help the training of the main objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import library and define python3 as default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\" File to solve the first miniproject which is classification \"\"\"\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import dlc_practical_prologue as prologue\n",
    "\n",
    "\n",
    "__author__ = 'Eugène Lemaitre, Natalie Bolón Brun, Louis Munier'\n",
    "__version__ = '0.1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import and Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(N, normalize):\n",
    "    \"\"\"Function to import dataset from prologue\"\"\"\n",
    "    train_input, train_target, train_classes, test_input, test_target, test_classes = prologue.generate_pair_sets(N)\n",
    "\n",
    "    # Normalize data\n",
    "    if normalize:\n",
    "        mu, std = train_input.mean(), train_input.std()\n",
    "        train_input.sub_(mu).div_(std)\n",
    "        test_input.sub_(mu).div_(std)\n",
    "\n",
    "    return train_input, train_classes, train_target, test_input, test_classes, test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(tensor):\n",
    "    one_hot = torch.zeros((tensor.size(0), 10)).type(torch.FloatTensor)\n",
    "    one_hot[list(range(0,tensor.size(0))), tensor] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_validation(data, device, size=200):\n",
    "    \"\"\"Function to split the data into up and down categories with also validation set.\"\"\"\n",
    "    rnd = []\n",
    "    up, down = 0, 1\n",
    "    valid_idx = []\n",
    "    train_idx = list(range(0, data.input.size(0)))\n",
    "    \n",
    "    for i in range(size):\n",
    "        rnd = random.randint(0, len(train_idx)-1)\n",
    "        valid_idx.append(train_idx.pop(rnd))\n",
    "    \n",
    "    # Create class variables to well store all the data\n",
    "    data_up = DataStoredValid(data.input[train_idx, up, :, :].reshape(len(train_idx), 1, data.input.size(2), data.input.size(3)).to(device),\n",
    "                              data.classes[train_idx, up].to(device),\n",
    "                              data.target[train_idx].type(torch.FloatTensor).to(device),\n",
    "                              data.input[valid_idx, up, :, :].reshape(len(valid_idx), 1, data.input.size(2), data.input.size(3)).to(device),\n",
    "                              data.classes[valid_idx, up].to(device),\n",
    "                              data.target[valid_idx].type(torch.FloatTensor).to(device))\n",
    "    \n",
    "    data_down = DataStoredValid(data.input[train_idx, down, :, :].reshape(len(train_idx), 1, data.input.size(2), data.input.size(3)).to(device),\n",
    "                                data.classes[train_idx, down].to(device),\n",
    "                                data.target[train_idx].type(torch.FloatTensor).to(device),\n",
    "                                data.input[valid_idx, down, :, :].reshape(len(valid_idx), 1, data.input.size(2), data.input.size(3)).to(device),\n",
    "                                data.classes[valid_idx, down].to(device),\n",
    "                                data.target[valid_idx].type(torch.FloatTensor).to(device))\n",
    "    \n",
    "    return data_up, data_down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Classes to well store data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataStored:\n",
    "    \"\"\"A class to well store data to have a cleaner code.\"\"\"\n",
    "    def __init__(self, data_in, classes, target):\n",
    "        self.input = data_in\n",
    "        self.classes = classes\n",
    "        self.target = target\n",
    "\n",
    "\n",
    "class DataStoredValid(DataStored):\n",
    "    \"\"\"\"A class to well store data with validation set to have a cleaner code.\"\"\"\n",
    "    def __init__(self, data_in, classes, target, valid_input, valid_classes, valid_target):\n",
    "        DataStored.__init__(self, data_in, classes, target)\n",
    "        self.valid_input = valid_input\n",
    "        self.valid_classes = valid_classes\n",
    "        self.valid_target = valid_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define the device to work on CUDA if it is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_device():\n",
    "    \"\"\"Check if cuda is available to run model on it.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        \n",
    "    print('\\nDevice : {}'.format(device))\n",
    "    \n",
    "    return device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_recognition(nn.Module):\n",
    "    \"\"\"Recognition model definition.\"\"\"\n",
    "    def __init__(self, nb_hidden):\n",
    "        super(Net_recognition, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(256, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2, stride=1))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        x = F.relu(self.fc1(x.view(-1, 256)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_compare(nn.Module):\n",
    "    \"\"\"Comparison model definition.\"\"\"\n",
    "    def __init__(self, nb_hidden):\n",
    "        super(Net_compare, self).__init__()\n",
    "        #self.conv1 = nn.Conv1d(1, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(20, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = F.relu(F.max_pool1d(self.conv1(x), kernel_size=3, stride=1))\n",
    "        x = F.relu(self.fc1(x.view(-1, 20)))\n",
    "        x = self.fc2(x).sigmoid()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Define training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_recognition(model, criterion, data, e,\n",
    "                            epochs = 25, mini_batch_size = 100, lr = 1e-1, verbose = 2):\n",
    "    \"\"\"Training recognition model.\"\"\"\n",
    "    sum_loss = 0\n",
    "    output_to_train = torch.zeros([data.input.size(0), 10])\n",
    "\n",
    "    for b in range(0, data.input.size(0), mini_batch_size):\n",
    "        output = model(data.input.narrow(0, b, mini_batch_size))\n",
    "        loss = criterion(output, data.classes.narrow(0, b, mini_batch_size))\n",
    "        \n",
    "        output_to_train[b:b+mini_batch_size, :] = output.detach()\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        sum_loss += loss.item()\n",
    "\n",
    "        for p in model.parameters():\n",
    "            p.data.sub_(lr * p.grad.data)\n",
    "\n",
    "    if verbose == 0: print('Epoch: {}, loss: {:0.2f}'.format(e+1, sum_loss), end = '\\t\\t\\r')\n",
    "    elif verbose == 1 and e%5 == 0: print(e+1, sum_loss)\n",
    "\n",
    "    nb_errors = compute_nb_errors_recognition(model, data.valid_input, data.valid_classes, mini_batch_size)\n",
    "    return nb_errors, output_to_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_compare(model, criterion, train, data, e,\n",
    "                        epochs = 25, mini_batch_size = 100, lr = 1e-1, verbose = 2):\n",
    "    \"\"\"Training comparison model.\"\"\"\n",
    "    sum_loss = 0\n",
    "\n",
    "    for b in range(0, train.size(0), mini_batch_size):\n",
    "        output = model(train.narrow(0, b, mini_batch_size))\n",
    "        loss = criterion(output, data.target.narrow(0, b, mini_batch_size).long())\n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        sum_loss += loss.item()\n",
    "        \n",
    "        for p in model.parameters():\n",
    "            p.data.sub_(lr * p.grad.data)\n",
    "\n",
    "    if verbose == 0: print('Epoch: {}, loss: {:0.2f}'.format(e+1, sum_loss), end = '\\t\\t\\r')\n",
    "    elif verbose == 1 and e%5 == 0: print(e+1, sum_loss)\n",
    "\n",
    "    nb_errors = compute_nb_errors_compare(model, train, data.valid_target, mini_batch_size)\n",
    "    return nb_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_architecture(data, device, criterion, validation_size, nb_mid_error, nb_final_error,\n",
    "                       repeat, r, epochs = 25, lr = 1e-2, verbose = 2):\n",
    "    print('Run : {}'.format(r+1))\n",
    "    \n",
    "    criterion_recognition = nn.CrossEntropyLoss().to(device)\n",
    "    # Define models\n",
    "    model_recognition = Net_recognition(hidden_layer).to(device)\n",
    "    model_compare = Net_compare(hidden_layer).to(device)\n",
    "    \n",
    "    # Process data\n",
    "    data_up, data_down = split_data_validation(data, device, size=validation_size)\n",
    "    output_to_train = torch.zeros([data_up.classes.size(0), 1, 20])\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        # Training models\n",
    "        nb_mid_error[e, r], output_to_train[:, 0, :10] = train_model_recognition(model_recognition, criterion_recognition, data_up, e, verbose=verbose)\n",
    "        nb_mid_error[e + epochs, r], output_to_train[:, 0, 10:] = train_model_recognition(model_recognition, criterion_recognition, data_down, e, verbose=verbose)\n",
    "        \n",
    "        nb_final_error = train_model_compare(model_compare, criterion, output_to_train, data_up, e)\n",
    "        \n",
    "\n",
    "        # Dynamically adapt learning rate\n",
    "        #dyn_lr = adapt_learning_rate(dyn_lr, loss, e)\n",
    "    \n",
    "    print('\\n')\n",
    "    return nb_mid_error, nb_final_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Compute number of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors_recognition(model, data_in, data_target, mini_batch_size = 100):\n",
    "    errors = 0\n",
    "    \n",
    "    for b in range(0, data_target.size(0), mini_batch_size):\n",
    "        output = model(data_in.narrow(0, b, mini_batch_size))\n",
    "        _, predicted = output.data.max(1)\n",
    "        \n",
    "        for k in range(mini_batch_size):\n",
    "            if data_target.narrow(0, b, mini_batch_size).data[k] != predicted[k] :\n",
    "                errors = errors + 1\n",
    "    \n",
    "    return errors*100/data_target.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors_compare(model, data_in, data_target, mini_batch_size = 100):\n",
    "    errors = 0\n",
    "    \n",
    "    for b in range(0, data_target.size(0), mini_batch_size):\n",
    "        output = model(data_in.narrow(0, b, mini_batch_size))\n",
    "        _, predicted = output.data.max(1)\n",
    "        \n",
    "        for k in range(mini_batch_size):\n",
    "            if (data_target.narrow(0, b, mini_batch_size).data[k] - predicted[k]).abs() <= 0.5 :\n",
    "                errors = errors + 1\n",
    "    \n",
    "    return errors*100/data_target.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Usefull functions to improve learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_learning_rate(learning_rate, loss, e):\n",
    "    \"\"\"Adapt the leaning rate: divide by two if oscillations are seen, multiply by two if a plateau is reached\n",
    "\n",
    "    e: epochs\n",
    "    \"\"\"\n",
    "    lst = loss[e - 50:e]\n",
    "    \n",
    "    if e > 50:\n",
    "        # Decreases learning rate if high variation in the loss\n",
    "        if loss[e] - loss[e - 1] - 0.5 > 0 and e > 5:\n",
    "            learning_rate = learning_rate/2\n",
    "            #optimizer = optim.SGD(model.parameters(), lr = learning_rate)\n",
    "            #print('Decrease learning rate ',learning_rate)\n",
    "\n",
    "        # Increases learning rate if a plateau is reached\n",
    "        elif abs(sum(lst)/len(lst) - loss[e]) < 0.05:\n",
    "            learning_rate = 2*learning_rate\n",
    "            #optimizer = optim.SGD(model.parameters(), lr = learning_rate)\n",
    "            #print('Increase learning rate ', learning_rate)\n",
    "            \n",
    "    return learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(errors, str_results):\n",
    "    dim = 1\n",
    "    xdata = np.arange(1, np.shape(errors)[0]+1)\n",
    "\n",
    "    plt.figure(str_results)\n",
    "    plt.title(str_results)\n",
    "    plt.xlabel('Epochs [-]')\n",
    "    plt.ylabel('Errors [%]')\n",
    "\n",
    "    plt.plot(xdata, errors.mean(dim), 'r')\n",
    "    plt.fill_between(xdata,\n",
    "                     errors.mean(dim) - errors.std(dim),\n",
    "                     errors.mean(dim) + errors.std(dim),\n",
    "                     color='gray',\n",
    "                     alpha=0.2)\n",
    "\n",
    "    print(str_results)\n",
    "    print(\"\\n\\nLast mean error : {}%\".format(errors[-1].mean()))\n",
    "    print(\"Last standard deviation : {}\".format(errors[-1].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Device : cpu\n",
      "Run : 1\n",
      "Epoch: 25, loss: 0.14\t\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Define variables\n",
    "N, normalize = 1000, True\n",
    "hidden_layer = 200\n",
    "repeat, validation_size = 1, 200\n",
    "\n",
    "epochs, mini_batch_size, dyn_lr, verbose = 25, 100, 1e-1, 0\n",
    "device = define_device()\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# Import data\n",
    "train_input, train_classes, train_target, test_input, test_classes, test_target = import_data(N, normalize)\n",
    "train_original = DataStored(train_input, train_classes, train_target)\n",
    "test_original = DataStored(test_input, test_classes, test_target)\n",
    "\n",
    "# Main process\n",
    "nb_mid_error = torch.zeros([2*epochs, repeat])\n",
    "nb_final_error = torch.zeros([epochs, repeat])\n",
    "\n",
    "for r in range(repeat):\n",
    "    nb_mid_error, nb_final_error = train_architecture(train_original, device, criterion, validation_size, nb_mid_error,\n",
    "                                                      nb_final_error, repeat, r, epochs=epochs, verbose=verbose)\n",
    "\n",
    "#plot_results(nb_mid_error[:epochs, :], 'Intermediate results up')\n",
    "#plot_results(nb_mid_error[epochs:, :], 'Intermediate results down')\n",
    "#plot_results(nb_final_error, 'Final results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_final_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
